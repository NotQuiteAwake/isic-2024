{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/abdmental01/multimodel-isic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "\n",
    "import io\n",
    "import warnings\n",
    "import gc\n",
    "import sys\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from transformers import AutoModelForImageClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import category_encoders as ce\n",
    "\n",
    "import joblib\n",
    "\n",
    "OWN_INSTANCE = True\n",
    "SEED = 42\n",
    "n_splits = 3\n",
    "\n",
    "os.makedirs('gradboost', exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns present in test but not in train: set()\n",
      "Columns present in train but not in test: {'iddx_3', 'target', 'tbp_lv_dnn_lesion_confidence', 'lesion_id', 'iddx_4', 'mel_thick_mm', 'iddx_2', 'iddx_full', 'iddx_5', 'mel_mitotic_index', 'iddx_1'}\n",
      "CPU times: user 2.46 s, sys: 292 ms, total: 2.75 s\n",
      "Wall time: 2.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "test_metadata_file = '/kaggle/input/isic-2024-challenge/test-metadata.csv'\n",
    "train_metadata_file = '/kaggle/input/isic-2024-challenge/train-metadata.csv'\n",
    "\n",
    "if OWN_INSTANCE:\n",
    "    test_metadata_file = 'data/test-metadata.csv'\n",
    "    train_metadata_file = 'data/train-metadata.csv'\n",
    "\n",
    "test = pd.read_csv(test_metadata_file)\n",
    "train = pd.read_csv(train_metadata_file)\n",
    "\n",
    "#train.drop('isic_id',axis=1,inplace=True)\n",
    "#test.drop('isic_id',axis=1,inplace=True)\n",
    "\n",
    "test_columns = set(test.columns)\n",
    "train_columns = set(train.columns)\n",
    "\n",
    "diff_test_train = test_columns - train_columns\n",
    "diff_train_test = train_columns - test_columns\n",
    "\n",
    "if not diff_test_train and not diff_train_test:\n",
    "    print(\"Both DataFrames have the same columns.\")\n",
    "else:\n",
    "    print(\"Columns present in test but not in train:\", diff_test_train)\n",
    "    print(\"Columns present in train but not in test:\", diff_train_test)\n",
    "\n",
    "train.drop(columns=['iddx_4', 'mel_mitotic_index', 'iddx_1', 'lesion_id', 'tbp_lv_dnn_lesion_confidence',\n",
    "                    'iddx_5', 'mel_thick_mm', 'iddx_2', 'iddx_full', 'iddx_3'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.68 s, sys: 500 ms, total: 2.18 s\n",
      "Wall time: 2.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def fe(df):\n",
    "    \n",
    "    # a sort of eccentricity\n",
    "    df[\"lesion_size_ratio\"]=df[\"tbp_lv_minorAxisMM\"]/df[\"clin_size_long_diam_mm\"]\n",
    "    # another dimensionless measure of eccentricity (think circle / square)\n",
    "    df[\"lesion_shape_index\"]=df[\"tbp_lv_areaMM2\"]/(df[\"tbp_lv_perimeterMM\"]**2)\n",
    "    # contrast between hue inside and outside\n",
    "    df[\"hue_contrast\"]= (df[\"tbp_lv_H\"]-df[\"tbp_lv_Hext\"]).abs()\n",
    "    # contrast between luminance inside and outside\n",
    "    df[\"luminance_contrast\"]= (df[\"tbp_lv_L\"]-df[\"tbp_lv_Lext\"]).abs()\n",
    "    # LAB is another color space similar to RGB. delta's are inside v. outside.\n",
    "    df[\"lesion_color_difference\"]=np.sqrt(df[\"tbp_lv_deltaA\"]**2+df[\"tbp_lv_deltaB\"]**2+df[\"tbp_lv_deltaL\"]**2)\n",
    "    # both metrics increase when asymmetry is higher and are on scale 0-10\n",
    "    df[\"border_complexity\"]=df[\"tbp_lv_norm_border\"]+df[\"tbp_lv_symm_2axis\"]\n",
    "    # position on 3D TBP\n",
    "    df[\"3d_position_distance\"]=np.sqrt(df[\"tbp_lv_x\"]**2+df[\"tbp_lv_y\"]**2+df[\"tbp_lv_z\"]**2)\n",
    "    # another measure of irregularity...?\n",
    "    df[\"perimeter_to_area_ratio\"]=df[\"tbp_lv_perimeterMM\"]/df[\"tbp_lv_areaMM2\"]\n",
    "    # contrast between lesion and surrounding, values from 5-25 + color variation 0 - 10\n",
    "    df[\"lesion_visibility_score\"]=df[\"tbp_lv_deltaLBnorm\"]+df[\"tbp_lv_norm_color\"]\n",
    "    # both are location indicators\n",
    "    df[\"combined_anatomical_site\"]=df[\"anatom_site_general\"]+\"_\"+df[\"tbp_lv_location\"]\n",
    "    # only when both are large does a lesion score high on this (cf border_complexity)\n",
    "    df[\"symmetry_border_consistency\"]=df[\"tbp_lv_symm_2axis\"]*df[\"tbp_lv_norm_border\"]\n",
    "    # whether the variation in color is similar inside and outside lesion\n",
    "    df[\"color_consistency\"]=df[\"tbp_lv_stdL\"]/df[\"tbp_lv_stdLExt\"]\n",
    "    # interactions are just products\n",
    "    df[\"size_age_interaction\"]=df[\"clin_size_long_diam_mm\"]*df[\"age_approx\"]\n",
    "    # hue inside and color irregularity\n",
    "    df[\"hue_color_std_interaction\"]=df[\"tbp_lv_H\"]*df[\"tbp_lv_color_std_mean\"]\n",
    "    # three measures of irregularity combined.\n",
    "    df[\"lesion_severity_index\"]=(df[\"tbp_lv_norm_border\"]+df[\"tbp_lv_norm_color\"]+df[\"tbp_lv_eccentricity\"])/3\n",
    "    df[\"shape_complexity_index\"]=df[\"border_complexity\"]+df[\"lesion_shape_index\"]\n",
    "    # first three terms are average contrast, last term is contrast in immediately surrounding skin\n",
    "    df[\"color_contrast_index\"]=df[\"tbp_lv_deltaA\"]+df[\"tbp_lv_deltaB\"]+df[\"tbp_lv_deltaL\"]+df[\"tbp_lv_deltaLBnorm\"]\n",
    "    # the malignant lesions can be way longer and a log scale might better capture this\n",
    "    df[\"log_lesion_area\"]=np.log(df[\"tbp_lv_areaMM2\"]+1)\n",
    "    # perhaps lesion gorws in size with age.\n",
    "    df[\"normalized_lesion_size\"]=df[\"clin_size_long_diam_mm\"]/df[\"age_approx\"]\n",
    "    # internal and external hue averaged\n",
    "    df[\"mean_hue_difference\"]=(df[\"tbp_lv_H\"]+df[\"tbp_lv_Hext\"])/2\n",
    "    # combining inner contrast assuming Gaussisna\n",
    "    df[\"std_dev_contrast\"]=np.sqrt((df[\"tbp_lv_deltaA\"]**2+df[\"tbp_lv_deltaB\"]**2+df[\"tbp_lv_deltaL\"]**2)/3)\n",
    "    # combine metrics of color and shape, both could be more irregular for malignant\n",
    "    df[\"color_shape_composite_index\"]=(df[\"tbp_lv_color_std_mean\"]+df[\"tbp_lv_area_perim_ratio\"]+df[\"tbp_lv_symm_2axis\"])/3\n",
    "    df[\"3d_lesion_orientation\"]=np.arctan2(df[\"tbp_lv_y\"],df[\"tbp_lv_x\"])\n",
    "    df[\"overall_color_difference\"]=(df[\"tbp_lv_deltaA\"]+df[\"tbp_lv_deltaB\"]+df[\"tbp_lv_deltaL\"])/3\n",
    "    df[\"symmetry_perimeter_interaction\"]=df[\"tbp_lv_symm_2axis\"]*df[\"tbp_lv_perimeterMM\"]\n",
    "    # the larger this value, the larger the \"irregularity\"\n",
    "    df[\"comprehensive_lesion_index\"]=(df[\"tbp_lv_area_perim_ratio\"]+df[\"tbp_lv_eccentricity\"]+df[\"tbp_lv_norm_color\"]+df[\"tbp_lv_symm_2axis\"])/4\n",
    "    \n",
    "    # categorical columns\n",
    "    n_cat = [\"combined_anatomical_site\"]\n",
    "    \n",
    "    return df, n_cat\n",
    "\n",
    "train, n_cat = fe(train)\n",
    "test, _ = fe(test)\n",
    "\n",
    "# columns with categories\n",
    "cat_cols = [\"sex\", \"tbp_tile_type\", \"tbp_lv_location\", \"tbp_lv_location_simple\",'patient_id',\n",
    "   'anatom_site_general','copyright_license','attribution','image_type'] + n_cat\n",
    "\n",
    "# drop columns only present in one set\n",
    "def align_columns(train, test):\n",
    "    common_cols = train.columns.intersection(test.columns)\n",
    "    train = train[common_cols]\n",
    "    test = test[common_cols]\n",
    "    return train, test\n",
    "\n",
    "# target will be removed by align_columns anyway, remove first and add back later.\n",
    "target = train['target']\n",
    "train_features = train.drop(columns=['target'], errors='ignore')\n",
    "\n",
    "train_features_aligned, test_features_aligned = align_columns(train_features, test)\n",
    "\n",
    "encoder = ce.OrdinalEncoder(cols=cat_cols, handle_unknown='ignore')\n",
    "train = encoder.fit_transform(train_features_aligned)\n",
    "# a second call to encoder.transform will apply the same statistics of fit_transform.\n",
    "test = encoder.transform(test_features_aligned)\n",
    "\n",
    "train.drop(columns=['isic_id'], inplace = True)\n",
    "test.drop(columns=['isic_id'], inplace = True)\n",
    "\n",
    "train['target'] = target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ViT and extract feature from last hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 401059/401059 [00:54<00:00, 7299.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2915.26 MiB, increment: 1274.11 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 315.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.8 s, sys: 3.99 s, total: 56.8 s\n",
      "Wall time: 56.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_path = '/kaggle/input/vitmae/transformers/default/1/'\n",
    "hdf_test_path = '/kaggle/input/isic-2024-challenge/test-image.hdf5'\n",
    "hdf_train_path = '/kaggle/input/isic-2024-challenge/train-image.hdf5'\n",
    "\n",
    "if OWN_INSTANCE:\n",
    "    model_path = 'TobanDjan/vitmae-sup'\n",
    "    hdf_test_path = 'data/test-image.hdf5'\n",
    "    hdf_train_path = 'data/train-image.hdf5'\n",
    "\n",
    "# Function to load images from encoded data\n",
    "def load_image_from_encoded_data(encoded_data):\n",
    "    image = Image.open(io.BytesIO(encoded_data))\n",
    "    return image.convert('RGB')\n",
    "\n",
    "# Define a custom Dataset for the HDF5 images\n",
    "class HDF5TestDataset(Dataset):\n",
    "    def __init__(self, image_data, ids, transform=None):\n",
    "        self.image_data = image_data\n",
    "        self.ids = ids\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_data = self.image_data[idx]\n",
    "        image = load_image_from_encoded_data(image_data)\n",
    "        #imshow(image)\n",
    "        #plt.show()\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # print(image.element_size() * image.nelement())\n",
    "        # 602112 B = 0.574 MB\n",
    "        return image, self.ids[idx]\n",
    "\n",
    "def get_dataset(hdf_file_path):\n",
    "    with h5py.File(hdf_file_path, 'r') as f:\n",
    "        image_data = [f[image_id][()] for image_id in tqdm(f.keys())]\n",
    "        ids = list(f.keys())\n",
    "        dataset = HDF5TestDataset(image_data=image_data, ids=ids, transform=val_transform)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "%memit train_dataset = get_dataset(hdf_train_path)\n",
    "test_dataset = get_dataset(hdf_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3117.21 MiB, increment: 65.04 MiB\n",
      "   patient_id  age_approx  sex  anatom_site_general  clin_size_long_diam_mm  \\\n",
      "0         NaN        45.0  1.0                  3.0                    2.70   \n",
      "1         NaN        35.0  2.0                  1.0                    2.52   \n",
      "2         NaN        65.0  1.0                  3.0                    3.16   \n",
      "\n",
      "   image_type  tbp_tile_type  tbp_lv_A  tbp_lv_Aext  tbp_lv_B  tbp_lv_Bext  \\\n",
      "0         1.0            2.0  22.80433    20.007270  28.38412    27.043640   \n",
      "1         1.0            2.0  16.64867     9.657964  31.31752    27.524318   \n",
      "2         1.0            2.0  24.25384    19.937380  30.46368    28.384240   \n",
      "\n",
      "    tbp_lv_C  tbp_lv_Cext   tbp_lv_H  tbp_lv_Hext  tbp_lv_L  tbp_lv_Lext  \\\n",
      "0  36.410100    33.640000  51.220960    53.505430  24.97985    31.114600   \n",
      "1  35.467806    29.169579  62.004494    70.664619  59.90409    68.141071   \n",
      "2  38.939500    34.686660  51.474730    54.915410  35.81945    41.358640   \n",
      "\n",
      "   tbp_lv_areaMM2  tbp_lv_area_perim_ratio  tbp_lv_color_std_mean  \\\n",
      "0        3.846876                22.907010               0.461149   \n",
      "1        2.120473                18.957821               0.000000   \n",
      "2        3.396510                19.464400               0.251236   \n",
      "\n",
      "   tbp_lv_deltaA  tbp_lv_deltaB  tbp_lv_deltaL  tbp_lv_deltaLB  \\\n",
      "0       2.797056       1.340481      -6.134747        6.436557   \n",
      "1       6.990705       3.793202      -8.236981        9.151127   \n",
      "2       4.316465       2.079433      -5.539191        6.041092   \n",
      "\n",
      "   tbp_lv_deltaLBnorm  tbp_lv_eccentricity  tbp_lv_location  \\\n",
      "0            6.843057             0.664465              3.0   \n",
      "1            6.083388             0.926698              6.0   \n",
      "2            5.446997             0.894776              3.0   \n",
      "\n",
      "   tbp_lv_location_simple  tbp_lv_minorAxisMM  tbp_lv_nevi_confidence  \\\n",
      "0                     3.0            2.187644            1.698104e-02   \n",
      "1                     6.0            1.032666            2.107364e-01   \n",
      "2                     3.0            1.520786            8.052259e-13   \n",
      "\n",
      "   tbp_lv_norm_border  tbp_lv_norm_color  tbp_lv_perimeterMM  \\\n",
      "0            5.435366           1.143374            9.387248   \n",
      "1            4.322201           0.000000            6.340311   \n",
      "2            3.968912           0.721739            8.130868   \n",
      "\n",
      "   tbp_lv_radial_color_std_max  tbp_lv_stdL  tbp_lv_stdLExt  \\\n",
      "0                     0.304827     1.281532        2.299935   \n",
      "1                     0.000000     1.271940        2.011223   \n",
      "2                     0.230742     1.080308        2.705857   \n",
      "\n",
      "   tbp_lv_symm_2axis  tbp_lv_symm_2axis_angle   tbp_lv_x     tbp_lv_y  \\\n",
      "0           0.479339                     20.0 -155.06510  1511.222000   \n",
      "1           0.426230                     25.0 -112.36924   629.535889   \n",
      "2           0.366071                    110.0  -84.29282  1303.978000   \n",
      "\n",
      "     tbp_lv_z  attribution  copyright_license  lesion_size_ratio  \\\n",
      "0  113.980100          1.0                1.0           0.810239   \n",
      "1  -15.019287          5.0                1.0           0.409788   \n",
      "2  -28.576050          NaN                1.0           0.481261   \n",
      "\n",
      "   lesion_shape_index  hue_contrast  luminance_contrast  \\\n",
      "0            0.043655      2.284470            6.134750   \n",
      "1            0.052749      8.660125            8.236981   \n",
      "2            0.051376      3.440680            5.539190   \n",
      "\n",
      "   lesion_color_difference  border_complexity  3d_position_distance  \\\n",
      "0                 6.874266           5.914705           1523.426592   \n",
      "1                11.450162           4.748430            639.662302   \n",
      "2                 7.323834           4.334983           1307.012048   \n",
      "\n",
      "   perimeter_to_area_ratio  lesion_visibility_score  combined_anatomical_site  \\\n",
      "0                 2.440226                 7.986431                       3.0   \n",
      "1                 2.990046                 6.083388                       6.0   \n",
      "2                 2.393889                 6.168736                       3.0   \n",
      "\n",
      "   symmetry_border_consistency  color_consistency  size_age_interaction  \\\n",
      "0                     2.605382           0.557204                 121.5   \n",
      "1                     1.842249           0.632421                  88.2   \n",
      "2                     1.452905           0.399248                 205.4   \n",
      "\n",
      "   hue_color_std_interaction  lesion_severity_index  shape_complexity_index  \\\n",
      "0                  23.620479               2.414402                5.958360   \n",
      "1                   0.000000               1.749633                4.801179   \n",
      "2                  12.932295               1.861809                4.386359   \n",
      "\n",
      "   color_contrast_index  log_lesion_area  normalized_lesion_size  \\\n",
      "0              4.845847         1.578334                0.060000   \n",
      "1              8.630314         1.137985                0.072000   \n",
      "2              6.303704         1.480811                0.048615   \n",
      "\n",
      "   mean_hue_difference  std_dev_contrast  color_shape_composite_index  \\\n",
      "0            52.363195          3.968859                     7.949166   \n",
      "1            66.334557          6.610754                     6.461350   \n",
      "2            53.195070          4.228418                     6.693902   \n",
      "\n",
      "   3d_lesion_orientation  overall_color_difference  \\\n",
      "0               1.673048                 -0.665737   \n",
      "1               1.747431                  0.848975   \n",
      "2               1.635349                  0.285569   \n",
      "\n",
      "   symmetry_perimeter_interaction  comprehensive_lesion_index  vitmae_target  \\\n",
      "0                        4.499672                    6.298547       0.001821   \n",
      "1                        2.702428                    5.077687       0.000295   \n",
      "2                        2.976478                    5.361747       0.000529   \n",
      "\n",
      "          0         1         2         3         4         5         6  \\\n",
      "0 -0.512342  5.498774  6.265830 -3.988366  2.402268 -2.839344  5.367242   \n",
      "1  0.011884  5.076280  3.981464 -6.945976  3.802044 -5.383007  6.560416   \n",
      "2 -0.778016  8.618467  3.214290 -5.375680  1.919163 -4.186031  4.043431   \n",
      "\n",
      "          7         8         9        10        11        12        13  \\\n",
      "0  0.756559 -1.812941  3.798150  7.561733  7.251655 -1.908271  4.718406   \n",
      "1  0.233549 -3.283330  4.693425  7.108475  6.404843 -0.635543  9.599909   \n",
      "2 -3.443126 -2.814883  2.109852  5.788032  6.858769 -1.984813  5.208786   \n",
      "\n",
      "         14        15        16        17        18        19         20  \\\n",
      "0  3.704540  5.946133  5.468503  3.818116  6.186650  6.451215 -10.541155   \n",
      "1  8.835879  4.551809  4.317307  5.293145  4.242517  8.957962  -5.012465   \n",
      "2  7.372383  1.936586  6.554333  4.245179  6.574463  7.397539  -9.660744   \n",
      "\n",
      "         21        22        23        24        25        26        27  \\\n",
      "0 -0.902930  9.603882 -1.741360 -0.904793 -2.007605 -1.245759 -4.224169   \n",
      "1 -0.375708  6.203680  1.639124 -6.641282 -1.305969 -1.109269 -5.519022   \n",
      "2 -0.213464  9.263803 -1.243818 -3.762406 -4.119922 -1.800617 -6.805574   \n",
      "\n",
      "         28        29        30        31        32        33        34  \\\n",
      "0 -0.583781  3.045177 -0.837671 -1.167437 -0.875750  7.961045  1.052885   \n",
      "1  0.287238  3.850667  0.572621  0.744795  4.344416  8.776666  1.341913   \n",
      "2 -0.392838  4.472922  2.792759 -1.048635  4.923357  9.039747 -0.109747   \n",
      "\n",
      "         35        36        37        38        39        40        41  \\\n",
      "0  2.855337 -0.746947  4.630784  9.479452  0.025844 -0.542037 -0.575085   \n",
      "1  3.994704 -0.471813  7.285702  9.308788  2.549449  4.105645  0.140647   \n",
      "2  0.691859  0.256645  7.275206  8.091113  0.030739  1.600224 -2.356475   \n",
      "\n",
      "         42        43        44        45        46        47        48  \\\n",
      "0  3.465559 -1.328653  1.600975  8.585629  1.514670 -2.554846  0.309518   \n",
      "1  3.015863 -1.338394  0.966199  6.995423  5.243013 -5.981434  2.827691   \n",
      "2  0.147406  0.628141  3.667644  6.227974  1.411983 -2.281489 -2.505606   \n",
      "\n",
      "         49        50        51        52        53        54        55  \\\n",
      "0  4.612245  4.543035  5.104222  1.862025  4.179881  0.477361  1.756798   \n",
      "1  4.457328  9.673126  3.131665 -3.759988  5.296070 -2.440183  0.303513   \n",
      "2  2.391961  4.036571  5.324059  3.386896  2.983724 -1.944211 -1.277459   \n",
      "\n",
      "         56        57         58        59        60        61        62  \\\n",
      "0 -5.246243 -4.880527   6.942151  3.382178  1.855054  1.350441 -0.273605   \n",
      "1 -3.100874 -8.003802   5.138001 -0.971271 -1.796757  0.144040  0.638453   \n",
      "2 -7.213227 -4.986742  11.705617  1.649046  2.465521 -3.490880 -0.101608   \n",
      "\n",
      "         63        64        65        66        67        68        69  \\\n",
      "0 -2.735558 -1.673445  7.921638 -5.281658  3.900811  1.524717  4.976750   \n",
      "1  2.750057 -2.265994  5.836282 -7.321799  0.936372 -2.080799  8.243387   \n",
      "2  3.279115 -2.060499  5.000424 -5.462626  3.378400 -0.286433  8.458691   \n",
      "\n",
      "         70        71        72        73        74        75        76  \\\n",
      "0  3.552482 -2.807628  1.638815 -1.299908 -7.422481  3.097558  0.836439   \n",
      "1  3.641931 -2.858932 -2.187186 -2.121626 -4.837830  1.182273  2.737319   \n",
      "2  2.285784 -2.637535  2.606908 -1.903381 -3.285037  1.285311  4.730067   \n",
      "\n",
      "         77        78         79        80        81        82        83  \\\n",
      "0  0.883922  1.400876  -8.708220 -4.037067  2.525001 -2.031183 -0.280205   \n",
      "1  5.336092 -1.235837  -9.525812  2.128442  2.900434  0.055154 -1.537125   \n",
      "2 -2.034863 -2.813537 -10.878402 -6.041597  3.938945  2.584302  1.065773   \n",
      "\n",
      "         84        85        86        87        88        89        90  \\\n",
      "0  1.666257 -7.426722  1.476593  3.778261 -1.015770 -5.055374 -8.025478   \n",
      "1  2.797880 -5.406919 -0.472192  6.228681 -2.294275 -4.903396 -5.094898   \n",
      "2  3.115897 -7.827197  3.506308  1.495938 -0.915259 -3.641130 -1.501826   \n",
      "\n",
      "         91        92        93        94        95        96        97  \\\n",
      "0  5.203905  7.135821 -1.812142  4.414479 -6.261444 -1.436520  3.356945   \n",
      "1  5.489552  7.172558 -1.551796  9.177359 -4.168057  2.454168  2.197504   \n",
      "2  4.711863  7.163191  0.211503  2.952800 -2.829311 -0.443138  1.359195   \n",
      "\n",
      "         98        99       100       101       102       103       104  \\\n",
      "0  0.985950 -0.312746 -0.282216  8.402987  6.381016  2.236724  2.828698   \n",
      "1 -3.901598  1.167701  3.973485  3.810006  3.901672  6.292960 -2.804391   \n",
      "2 -6.320241 -3.428730  2.932396  5.594114  4.271670 -0.117714  4.549947   \n",
      "\n",
      "        105       106       107       108       109       110       111  \\\n",
      "0  2.524642  2.586679 -1.388151  4.888042 -2.119130  4.460756  3.751247   \n",
      "1  3.541455 -3.147662 -4.669468  8.415653  3.520339 -3.250444  4.196913   \n",
      "2  2.642758  1.236713 -1.366223  6.941113  0.704416  3.755700  2.872660   \n",
      "\n",
      "        112       113       114       115       116       117       118  \\\n",
      "0  3.435591  8.419958 -1.064406 -5.010780 -5.988576  4.713245  3.279997   \n",
      "1 -0.565209  6.454134 -1.595183  0.092748 -6.366392  2.244274 -2.084439   \n",
      "2 -1.958146  6.962231  0.659789 -1.935888 -8.431801  3.681091 -0.823050   \n",
      "\n",
      "        119       120       121        122       123       124       125  \\\n",
      "0  2.190382  0.395668  3.032079 -11.821008  2.692669 -0.854031  1.794160   \n",
      "1  7.719293 -2.097365 -1.383193 -11.342415  3.109077  3.050765  1.332776   \n",
      "2  5.089346 -0.147752 -0.528598  -8.400887  3.361966  2.874515  4.877967   \n",
      "\n",
      "        126       127       128       129       130       131       132  \\\n",
      "0  0.015780 -2.161886  5.659170  4.186955  3.599927  3.306658  0.090143   \n",
      "1  1.969249 -0.688024  7.086177  5.106133  1.755898  5.506168  1.723917   \n",
      "2  1.747120 -3.576091  5.904081  7.292677  3.039963  3.841420 -2.480597   \n",
      "\n",
      "        133       134       135       136       137        138       139  \\\n",
      "0  6.568746  1.297155  0.030116 -0.033933  1.007942   9.982763  2.600752   \n",
      "1  4.651663  2.559913  3.258492  1.046398  2.605344   8.352418  3.168463   \n",
      "2  5.066056  0.662438  3.851055 -3.326110  0.839883  15.308919  2.067680   \n",
      "\n",
      "        140       141       142       143        144       145       146  \\\n",
      "0  4.112705  2.129717  2.536918  5.912672  -9.290344 -1.174029  0.704263   \n",
      "1  3.351998  2.687027  3.712698  8.638689  -5.704795  2.886477  5.348046   \n",
      "2  3.219923 -2.653446  2.358588  6.682324 -13.104767  2.438964  1.351512   \n",
      "\n",
      "        147       148       149       150        151       152       153  \\\n",
      "0 -0.743617  2.378557  1.140511  6.323403 -10.346106 -1.168698 -8.575945   \n",
      "1  3.194162  3.328543  3.030454  3.711481  -7.819548 -3.077468 -5.035323   \n",
      "2  4.757973  3.377977 -1.190158  4.427925  -8.479651  0.948408 -4.126768   \n",
      "\n",
      "        154       155       156       157       158       159        160  \\\n",
      "0 -2.149914 -1.155551  6.722228 -1.769907  0.088227 -5.193819  11.422987   \n",
      "1  0.413867 -0.183629  8.716199 -0.108843 -0.653249 -9.586213  11.490108   \n",
      "2 -2.656227  0.881956  9.089529 -1.031581  0.931466 -4.020546   8.363848   \n",
      "\n",
      "        161       162       163       164        165       166       167  \\\n",
      "0  2.373849  2.529332 -0.347360 -2.436768  13.386779  4.822845 -3.890884   \n",
      "1  2.279403 -1.409308 -3.803487  3.217902  10.366066  3.227063 -5.008868   \n",
      "2 -0.543818 -1.260683  1.399109 -1.346590  13.029305  4.307100 -2.414456   \n",
      "\n",
      "        168       169       170       171       172        173       174  \\\n",
      "0  5.705550 -1.665167 -4.633592 -6.264931  9.286403  10.803887  4.512214   \n",
      "1  4.708467 -1.073161 -3.449650 -4.085712  3.436133  10.183681  7.116304   \n",
      "2  5.141710 -1.991091 -3.952850 -5.665846  6.927544   6.835852  2.902688   \n",
      "\n",
      "        175        176       177       178       179       180       181  \\\n",
      "0  3.645533   0.651029 -7.430291 -2.127964  2.672200  6.634837 -2.031816   \n",
      "1  3.169276   6.656532 -7.669874  1.820344  0.634426  2.899315  0.448461   \n",
      "2  4.880750  10.559583 -7.072498  0.510988  1.779860  9.101938 -4.076750   \n",
      "\n",
      "        182       183       184       185       186       187       188  \\\n",
      "0  6.142060  0.102387  2.279007 -5.866715  3.725241  0.899106  1.218920   \n",
      "1  3.744208  2.979871  1.737819 -7.599245  3.372398  2.673086 -2.987082   \n",
      "2  0.947345 -0.177447  4.386582 -5.225199  3.234244 -0.273857 -2.097907   \n",
      "\n",
      "        189       190       191       192       193       194       195  \\\n",
      "0  7.670821  0.694000  6.730416  4.616413 -2.329665  0.054311  1.252207   \n",
      "1  5.517822 -0.377524  2.821843  3.081430 -2.698139 -3.151134  5.080462   \n",
      "2  8.932621 -3.557444  7.913318  5.628481 -1.512957 -2.279302  3.677805   \n",
      "\n",
      "        196       197       198       199       200       201        202  \\\n",
      "0 -5.339629 -2.445426 -2.616760 -3.878759 -3.997475 -0.779398  14.323123   \n",
      "1 -5.253121 -8.799831 -1.543303 -4.041335 -6.535139 -1.262285  13.450891   \n",
      "2 -4.046399 -0.802719  1.359996 -2.224543 -2.650342 -0.203507  12.122980   \n",
      "\n",
      "        203       204       205        206       207       208       209  \\\n",
      "0 -4.129714 -1.791968  3.287231  -6.160136  0.900563 -0.144477 -2.827348   \n",
      "1 -5.764323 -1.953235  2.602406  -9.184378  4.883999 -4.729039 -3.275540   \n",
      "2 -4.871232 -1.282857  5.659575 -11.046028  1.649191 -4.039043 -5.293837   \n",
      "\n",
      "        210       211       212       213       214       215       216  \\\n",
      "0 -6.626390  0.728501 -1.620937  7.333197 -3.516665  4.803118 -1.941631   \n",
      "1  0.203813 -4.688251 -0.080303  9.548987 -0.813179  1.364278 -0.712035   \n",
      "2 -3.389574 -4.400141 -2.861886  9.217276 -1.844291  6.393819  0.093472   \n",
      "\n",
      "        217       218       219       220       221       222       223  \\\n",
      "0  1.971584 -3.775892 -2.951075 -0.097724  3.701589 -2.528882 -6.556869   \n",
      "1  5.379955 -2.756647 -1.782200  2.079870  4.152403  0.070771 -7.940591   \n",
      "2 -0.479700  1.410894 -4.121969 -4.728966  2.404455 -6.102116 -4.919560   \n",
      "\n",
      "        224       225       226       227        228       229       230  \\\n",
      "0  2.667446 -0.621013 -1.213855  4.140936   9.521206  7.202415 -2.003875   \n",
      "1  4.903634 -1.284408  0.825889  2.642529   9.777747 -0.139540  1.207818   \n",
      "2  4.480707 -1.059006 -4.821074  0.406886  10.099938  3.362100  1.324510   \n",
      "\n",
      "        231        232        233       234       235       236       237  \\\n",
      "0 -2.627421  11.493814  -6.313848 -1.538387 -6.372820 -1.540151 -2.416831   \n",
      "1 -2.680833  10.328859  -6.367981  3.270174 -6.557365 -7.498055 -0.003263   \n",
      "2  1.284821   8.903049 -11.668399 -1.201099 -5.172985  0.778919  1.173668   \n",
      "\n",
      "        238       239       240       241       242       243       244  \\\n",
      "0 -6.606830  5.370549 -3.353601  0.918809 -2.149683  5.702981  3.280853   \n",
      "1 -7.133945  1.674740 -5.223971  5.666963  2.458434  4.245736  6.488455   \n",
      "2 -7.643207 -1.872437  1.241607  3.596739  1.240226 -2.005887  5.421600   \n",
      "\n",
      "        245       246       247       248       249       250       251  \\\n",
      "0  7.834673 -1.590754  5.467269  1.432276 -2.363866 -5.693571  4.048932   \n",
      "1  9.021879  0.136437  4.638530  6.900512 -2.920182 -0.696779  1.474715   \n",
      "2  5.260376 -4.639798  1.618050  3.740006 -6.045836 -1.707564  3.977593   \n",
      "\n",
      "        252       253       254        255       256        257       258  \\\n",
      "0  2.276694  0.177334 -1.512538   6.443739  4.892899   8.517827  3.245776   \n",
      "1  1.602569  3.915207 -0.168012  10.099085  4.885141  12.775506  3.888884   \n",
      "2  4.484835 -4.946472  1.332091   9.744003  4.397426   7.084490  5.637108   \n",
      "\n",
      "        259       260       261       262       263       264       265  \\\n",
      "0  8.196332 -2.127424 -0.231047  6.883487  4.178741  3.038683 -4.912256   \n",
      "1  9.808773 -1.980022 -2.306550  6.436012  4.918014  1.441703 -4.527997   \n",
      "2  6.658355 -1.739890  1.175801  3.696496  3.974824  1.492242 -2.500689   \n",
      "\n",
      "        266       267       268        269       270       271       272  \\\n",
      "0  1.426572  5.081780  2.641555  -1.016005  4.606812  4.475745 -2.812140   \n",
      "1  2.279931  6.131382  1.127260  15.074241 -0.352417  0.040617 -5.551861   \n",
      "2  3.733221  3.390877  2.843057   8.820299  4.514563  1.789941 -2.919040   \n",
      "\n",
      "        273       274       275       276       277       278       279  \\\n",
      "0 -3.397801 -0.082687 -3.051357  4.874689  0.213441  2.576867  0.534379   \n",
      "1 -1.641537 -0.136877  2.105265  7.209534  1.007130  2.827325  0.867912   \n",
      "2 -2.300895  0.344093 -1.356072  9.887792  3.249926  2.834062  1.885259   \n",
      "\n",
      "        280       281        282       283       284       285       286  \\\n",
      "0  7.656585  2.477826  -1.936557  4.243130  6.274116 -0.328598  3.720407   \n",
      "1  6.998644  4.662125 -11.012863  5.343256  3.683580  0.112248  3.384193   \n",
      "2  9.029957 -0.424742  -7.766192  5.369393  6.080565 -0.748727  1.955527   \n",
      "\n",
      "        287       288       289       290       291       292       293  \\\n",
      "0  1.516755  4.771420 -0.705925 -1.206001 -5.638780  4.696255  0.721528   \n",
      "1 -4.154713  5.477503 -1.355555 -5.544943 -4.514176  5.400519  2.848356   \n",
      "2  0.572237  3.773177  2.950163 -0.558937 -5.743187  7.078942  1.441893   \n",
      "\n",
      "        294       295        296       297       298       299       300  \\\n",
      "0  3.953022 -8.816081  -8.632902  1.884804 -2.383946  3.352233  3.964846   \n",
      "1 -0.714821  0.250670 -12.371244 -2.462976 -1.807740  6.830035  3.231056   \n",
      "2  2.019751 -7.429471 -14.240118 -0.126534 -2.337806  4.283927  2.352474   \n",
      "\n",
      "         301       302       303       304       305       306       307  \\\n",
      "0  10.506770  4.983538  0.810864 -0.116927  2.620514  2.190411  5.880701   \n",
      "1   4.979098  2.957650  2.985742 -3.016572 -2.256058  5.726044  1.653694   \n",
      "2   9.381109  2.502800  9.840799 -0.562623 -2.525675  2.137517  2.345128   \n",
      "\n",
      "        308       309       310       311       312       313       314  \\\n",
      "0 -3.284976  2.320523 -4.499937 -2.091884  2.969268 -0.470733  3.185645   \n",
      "1 -2.963932  3.556847 -1.191464 -1.150325  4.925622  4.510609  6.658770   \n",
      "2 -2.138541  3.775318 -7.306201 -0.323952  7.872405  0.954722  4.436567   \n",
      "\n",
      "        315       316       317       318       319       320        321  \\\n",
      "0  0.986730  5.170189  1.523729 -2.759785  0.900763  3.612392  11.442213   \n",
      "1 -1.070644  2.782727 -3.055183 -5.478908 -2.673925  4.121345   9.998279   \n",
      "2 -3.555663  5.948848 -1.487046 -4.591218  4.029442  4.107037  16.615971   \n",
      "\n",
      "        322       323       324        325       326       327       328  \\\n",
      "0  0.754271  4.935759  0.759151   9.323685 -0.559333 -3.890749 -1.834700   \n",
      "1  2.645334  4.893978  2.196852   7.556472 -1.578284 -6.914187  0.466591   \n",
      "2  0.682735  9.312649  0.571727  10.369120  2.074723 -5.450532 -0.969838   \n",
      "\n",
      "        329       330       331       332       333       334       335  \\\n",
      "0  1.660749  5.882014  3.245754  0.459264 -4.249404  1.007091  1.640270   \n",
      "1 -1.506546  9.372327  1.190741  0.880100 -3.479696 -2.950979  3.256687   \n",
      "2 -2.993637  8.225658  3.630315  5.815028 -4.790201 -2.173142  2.884615   \n",
      "\n",
      "         336       337       338       339       340       341       342  \\\n",
      "0   8.683000  3.056236  2.746969  3.091991 -0.253553  7.764925  2.571585   \n",
      "1  10.818004 -1.144510  4.787258  1.062632  0.980928  7.351124  5.247365   \n",
      "2  11.251170  6.124290  1.473000  3.542681  2.885342  5.372850  3.388490   \n",
      "\n",
      "        343       344       345       346       347       348       349  \\\n",
      "0 -0.234919 -6.582291  6.804104  4.326954 -3.517459 -5.078895 -3.443945   \n",
      "1 -0.059044 -1.385268  5.046400  2.645226  1.394190 -3.952779 -9.845604   \n",
      "2  0.454017 -0.884050  3.398673  0.806501 -1.735043 -4.253765 -5.220466   \n",
      "\n",
      "        350        351       352       353       354       355        356  \\\n",
      "0  0.588732   6.353188 -1.191726 -2.455800  0.473731  0.341188  -7.367589   \n",
      "1  3.291583   8.182332 -0.343758 -4.237495  2.455891  1.870464 -12.515316   \n",
      "2 -0.479938  11.571712  0.947838 -0.203879  6.301405  1.965168  -6.638496   \n",
      "\n",
      "         357       358       359       360       361        362        363  \\\n",
      "0  10.827466 -1.498062 -4.401371 -0.255149  5.479191   9.572740  15.579205   \n",
      "1   6.153667 -0.454132 -5.823822 -1.289349  5.463256  10.637396   7.049834   \n",
      "2   7.963473 -1.718420 -4.467536 -1.457854  5.705060  14.585754  14.672222   \n",
      "\n",
      "        364       365       366       367       368       369       370  \\\n",
      "0  2.357168 -4.257221 -0.946403 -0.901994  0.909496  2.236530  5.399170   \n",
      "1 -2.309557 -9.900827  0.160575 -4.376964 -1.014477  5.018772  6.287980   \n",
      "2  2.444045 -6.263052 -1.072601 -0.491673  0.047959  1.465990  6.233801   \n",
      "\n",
      "         371       372       373       374       375       376       377  \\\n",
      "0   3.764336  7.060910  0.081726 -2.382210  3.740088  8.187711 -5.618881   \n",
      "1   5.132640  8.123648  0.529090 -2.313103  0.738633  6.071724 -6.224301   \n",
      "2  10.500159  7.443384 -1.991267 -6.592315  2.264811  2.688986 -8.988352   \n",
      "\n",
      "        378       379       380       381       382       383       384  \\\n",
      "0  1.456340 -4.290213 -3.058264 -7.700046 -3.448586 -1.628085  3.981739   \n",
      "1 -0.171943 -3.975506 -4.958628 -2.167983  1.034521  0.271167  3.125262   \n",
      "2  0.869122 -2.601684 -5.217714 -4.478686 -4.865394  0.204132  0.609713   \n",
      "\n",
      "        385       386        387       388       389       390       391  \\\n",
      "0  0.014504  7.764053  -3.505615  3.829432  3.252479  2.207580 -3.762388   \n",
      "1  8.123637  4.619843  -9.838515  1.040187  4.537378  2.767924 -5.362264   \n",
      "2  3.586815  9.182950 -10.324893  5.791978  2.560758  0.145169 -6.037626   \n",
      "\n",
      "        392       393       394       395       396       397       398  \\\n",
      "0  3.704805 -7.981130  0.747436 -0.466111  4.095699  0.282996  0.689264   \n",
      "1  5.208903 -6.571039  0.827487 -3.444393  3.123785  0.879191  4.345235   \n",
      "2  4.682936 -5.029436  0.001937 -2.118857  2.983626  6.400429  0.196128   \n",
      "\n",
      "        399       400       401        402       403       404       405  \\\n",
      "0  7.384375  3.541142  1.953196  11.027538  4.431239  3.185033 -1.747195   \n",
      "1  3.244598  5.311085  3.731778  13.838221  2.699672  3.853113 -0.814761   \n",
      "2  4.332618  1.295987  2.175251  14.209248  0.171510  1.630501 -4.354728   \n",
      "\n",
      "        406       407        408       409       410       411       412  \\\n",
      "0  1.500862 -9.579712  14.151275  5.612884  4.962738  0.924026  1.458731   \n",
      "1  1.205217 -6.379550   6.701580  2.864459 -3.180168 -5.310359  2.509090   \n",
      "2  1.614140 -8.932818   7.944305  4.092144  3.697258 -1.925906  3.257120   \n",
      "\n",
      "        413       414       415       416       417       418       419  \\\n",
      "0  4.292596 -3.048800  1.934222  3.617919  1.159331  4.528063  2.627592   \n",
      "1  6.829544 -1.691213 -2.610354  1.738861  1.529398  0.574755  2.694277   \n",
      "2  6.727205 -4.468095 -1.331495  5.319496 -0.758341  6.219654 -2.309747   \n",
      "\n",
      "        420       421       422       423       424       425       426  \\\n",
      "0  5.246436 -1.540404 -1.425310 -0.832911  2.938728  4.251649  2.707447   \n",
      "1  4.659584 -2.505162 -3.548780 -1.589410  2.783145  6.743535  2.804597   \n",
      "2  4.480654 -2.564462 -1.683195 -5.120607  6.848409 -2.001151  2.826652   \n",
      "\n",
      "        427        428       429       430       431       432       433  \\\n",
      "0  3.234791  -6.872139 -2.849218 -5.652592  2.552338  1.708246  9.541713   \n",
      "1  5.046195 -11.363091 -3.443433 -7.283581  3.090659  2.098804  3.601264   \n",
      "2  1.081767  -8.836430 -2.856784 -3.536243  0.148224  1.312236  9.138739   \n",
      "\n",
      "        434       435       436       437       438        439       440  \\\n",
      "0 -3.071900  3.444695 -1.711762 -0.301595  2.987290 -10.337823 -0.049556   \n",
      "1  3.685186  2.562846 -6.969096 -1.937211  3.022188  -7.853242 -1.410622   \n",
      "2 -4.319039  1.498566 -5.375278  1.373832 -1.092698 -12.959636 -0.909496   \n",
      "\n",
      "        441        442       443       444       445        446       447  \\\n",
      "0 -6.506704  11.779023 -1.145273  4.892985  3.328372   7.928350 -0.781920   \n",
      "1 -2.161287  12.787434  3.296536  3.170882  0.788740  11.891200 -3.928144   \n",
      "2 -4.335858  14.395160  5.772525  5.691936  2.630753  10.283082 -2.362298   \n",
      "\n",
      "        448        449       450       451        452       453       454  \\\n",
      "0  3.546739  10.324690  3.460997 -1.244663   9.092966 -5.200515 -4.983803   \n",
      "1  5.384959   8.386749  4.042791 -3.608637  11.168404 -2.285012 -6.322003   \n",
      "2  1.041782   6.143801  5.049412 -2.602130  13.201207 -5.833457 -6.628772   \n",
      "\n",
      "        455       456        457       458       459       460       461  \\\n",
      "0  0.754264  1.711784  10.402021  0.466829 -2.005531  2.177423  0.031499   \n",
      "1  7.170629  1.208637   9.654655 -0.895756 -1.681102 -1.192629  0.486513   \n",
      "2  1.717411 -3.014991  12.171575 -1.573186 -3.523585  4.544349  2.835632   \n",
      "\n",
      "        462       463       464       465       466       467       468  \\\n",
      "0  8.049297 -3.728598  3.066345  4.101121  5.183088  5.693027  2.491184   \n",
      "1  1.536951 -1.043232  6.921479  3.330382  8.718792  7.625607 -0.629058   \n",
      "2  4.436632 -0.532417  3.608212  1.076789  4.605874  6.595045  4.583241   \n",
      "\n",
      "        469       470       471        472       473        474       475  \\\n",
      "0  6.327743  1.613642  5.032028 -13.721598  0.010905  11.087158 -5.595223   \n",
      "1  3.274647  2.822093 -1.931194  -7.840301  0.276278   9.886148 -0.239212   \n",
      "2  2.951386  2.428098  3.465846  -8.826183  5.479822  11.483584 -4.684621   \n",
      "\n",
      "        476        477       478       479       480       481       482  \\\n",
      "0 -1.443388   6.866642  7.711093 -5.897723 -2.579350 -3.250857  1.208765   \n",
      "1 -1.714910   8.096487  5.007948 -1.440939 -5.526867 -0.554444  0.445847   \n",
      "2 -1.926195  10.351822  8.253866 -7.218194 -5.389279 -6.007019  6.407625   \n",
      "\n",
      "        483       484       485       486       487        488       489  \\\n",
      "0 -4.158043  3.519829 -4.772115 -1.397394  3.121852 -21.305771  7.415612   \n",
      "1 -1.440453  1.669591 -6.955024 -1.998213  1.662475 -24.445658  6.815559   \n",
      "2 -3.894673  3.545908 -3.632144 -0.477214  1.033508  -7.494497  4.797710   \n",
      "\n",
      "        490       491       492       493       494       495       496  \\\n",
      "0 -2.251432  2.605695 -3.395562  6.271662 -4.971354 -4.518764  5.447398   \n",
      "1 -3.023092  3.505195 -2.774040  6.918365 -5.629951 -4.533830  7.060994   \n",
      "2  0.806135  0.939122  0.918197  6.875713 -6.257428 -6.974932  6.501214   \n",
      "\n",
      "        497       498       499       500       501       502       503  \\\n",
      "0 -3.849774  3.555894  1.465304 -1.759421  9.297342  5.045647  3.064045   \n",
      "1  2.100684  3.061943  2.840523  0.285688  6.817976  3.151401  2.433526   \n",
      "2 -0.364267  6.043126  5.715467  1.152841  6.404464  0.805061  5.141844   \n",
      "\n",
      "        504        505       506       507        508       509       510  \\\n",
      "0 -1.553087   7.193048  8.038042 -7.453479   3.809627 -5.433094 -1.178253   \n",
      "1 -0.352163   9.952498  3.359372 -1.855557  11.883650 -6.305484 -0.353445   \n",
      "2  0.313061  10.044941  4.966208 -5.895192   0.339228 -6.779512  0.590426   \n",
      "\n",
      "        511       512        513       514       515       516       517  \\\n",
      "0 -0.658204  2.226543   6.991565  3.748971 -0.270626  1.066001  1.910944   \n",
      "1  4.891483  4.383522   9.377987  7.349686 -2.893852  3.128098  2.782786   \n",
      "2  5.525647  1.092367  13.953909  2.800340  0.411009 -1.413273  0.786142   \n",
      "\n",
      "        518       519       520       521       522       523       524  \\\n",
      "0  1.555739 -0.993121  2.265959  0.236280 -3.650239  0.132367  9.187382   \n",
      "1  0.891828  3.284452  3.806560  0.542022 -3.288730 -0.290196  9.609091   \n",
      "2  4.995351 -0.527361  2.091135 -1.767543  2.729028  0.646768  9.378597   \n",
      "\n",
      "        525       526       527       528       529       530       531  \\\n",
      "0 -3.429739  8.018151  3.484296  0.655118 -9.188274  1.990722  5.060980   \n",
      "1 -1.317626  8.576121  0.656395  2.575962 -7.549140 -4.255413  3.631800   \n",
      "2 -2.258485  8.915151  0.850149  1.426846 -8.237798  2.572594  6.044697   \n",
      "\n",
      "        532       533       534       535       536       537        538  \\\n",
      "0 -2.188811  0.521446 -1.477528  3.044367  0.627410  3.881531  10.840718   \n",
      "1 -1.417916  3.159049 -0.266864  3.938340  1.071915  4.022372  11.350594   \n",
      "2 -6.136952  0.896393  0.633108  9.079800  0.775329  3.837631   7.286502   \n",
      "\n",
      "        539       540       541       542       543       544       545  \\\n",
      "0 -7.280124  4.844300  3.275946 -1.304130 -3.637319  2.675270 -1.072589   \n",
      "1 -5.815916  1.036153  5.631141 -0.460118 -2.706803  4.071708 -2.217155   \n",
      "2 -5.235377  2.754258  2.802927  0.971105 -3.217756  0.928379  0.412151   \n",
      "\n",
      "        546       547       548       549       550       551        552  \\\n",
      "0  9.409513  2.209998 -4.285268  0.094839 -1.408327  6.007827  11.052458   \n",
      "1  4.539707  4.493857 -2.647659 -7.037330  0.638184  0.349830   8.103945   \n",
      "2  4.519446  1.018260 -3.327475 -0.566058  1.229038  7.251291   8.616112   \n",
      "\n",
      "        553       554       555       556       557       558       559  \\\n",
      "0 -0.779025  3.790367  6.354476  3.527780  1.414162 -4.363219 -2.009728   \n",
      "1 -3.251729 -0.067040  7.642807  3.483007 -4.403882 -0.346558 -6.685835   \n",
      "2 -8.177920  3.406263  7.934041  5.592952  2.782945 -1.244080 -2.167802   \n",
      "\n",
      "        560       561       562       563       564       565       566  \\\n",
      "0 -0.743733 -5.086032 -0.592285  2.883341 -3.347496  4.144602  4.360988   \n",
      "1 -2.071039 -8.224617  4.334556  6.902608  1.486719  2.320609  2.907660   \n",
      "2 -1.571363 -5.315285  3.974639  6.487471 -2.345076  4.478126  9.100410   \n",
      "\n",
      "        567       568       569       570       571       572       573  \\\n",
      "0 -3.906962  5.562311  4.025024 -3.369622 -1.939618  3.784825  1.621525   \n",
      "1  1.023082  2.725866  2.278027 -3.064684 -0.267102  7.105965  0.079320   \n",
      "2  0.787181  4.603010  4.792126 -0.483582 -1.273827  2.634359  1.731794   \n",
      "\n",
      "        574       575       576       577       578       579       580  \\\n",
      "0  2.469263  3.823434 -0.915197  5.983989  3.060506  0.866560  2.322971   \n",
      "1  1.368805  2.088537  0.068085  2.350132 -1.249138  2.168449  2.968047   \n",
      "2  2.516301  3.229208 -1.495977 -0.550542  2.063306  0.664502  2.692454   \n",
      "\n",
      "        581       582       583       584        585       586       587  \\\n",
      "0  4.829021 -0.534223 -1.444396  0.411299  10.762456  1.954648 -4.452377   \n",
      "1  0.353840  6.271663 -1.234468 -0.420230  10.538379  3.930881 -4.091378   \n",
      "2  4.536657 -1.443482 -3.763798 -0.693672   8.143744 -1.442910 -5.648060   \n",
      "\n",
      "         588       589       590       591       592       593       594  \\\n",
      "0   9.144588 -4.842640  2.849414 -4.255326 -1.031695 -5.284551 -2.283350   \n",
      "1  12.577468 -6.011832  7.161708 -6.070146  0.553883 -3.049592 -0.390216   \n",
      "2  11.809929 -6.910387  0.930738 -6.355860  0.162187 -3.454333 -2.263672   \n",
      "\n",
      "        595       596       597       598       599        600        601  \\\n",
      "0 -4.510019  5.800364  0.869211  1.859183 -0.512611 -13.973656   9.198095   \n",
      "1 -5.575316  3.060148 -3.241237  1.866229 -0.639929 -14.579553  10.180712   \n",
      "2 -0.934197  1.388175  1.158518 -0.489791  2.792522 -15.633608   8.211802   \n",
      "\n",
      "        602       603       604       605       606       607        608  \\\n",
      "0  2.769539 -1.826089  6.806952 -2.551227  4.257380  4.853127  -4.484433   \n",
      "1  2.320599  0.582523  9.999882 -5.913277  2.944304  7.574914 -12.845409   \n",
      "2  3.635391  0.469872  6.343792  2.553983  5.920725  4.317519  -8.941416   \n",
      "\n",
      "        609        610       611       612       613       614       615  \\\n",
      "0  2.004831   2.627063 -3.161144 -5.405416  1.031298 -1.163482  3.932253   \n",
      "1 -1.878917  10.257089 -4.116736 -1.626933  2.055047 -0.631146  2.226192   \n",
      "2  2.180866   0.987382 -6.478734 -2.041762 -3.107208  4.195336  0.619232   \n",
      "\n",
      "        616        617       618       619       620       621       622  \\\n",
      "0  0.514693   7.562469  2.702662  6.998847  5.802335 -4.079314  1.652922   \n",
      "1  1.831114  10.078318  2.251768  7.441074  6.765052 -1.935665  6.474862   \n",
      "2 -0.728923  10.996165  2.199292  7.202709  8.359301 -3.486707  4.634755   \n",
      "\n",
      "        623        624        625       626       627       628        629  \\\n",
      "0  4.128378   7.550201   8.918146  4.931402 -8.861875  4.709006  -5.379605   \n",
      "1  4.331264  11.510175  10.149969  3.333204 -6.565543  9.483396  -3.603006   \n",
      "2  4.543770   5.361018   8.666249  2.451016 -4.316738  5.603684 -12.220755   \n",
      "\n",
      "        630       631       632       633       634       635       636  \\\n",
      "0  2.475394 -2.635018  2.907743  4.662490  9.558729  1.839234  0.440137   \n",
      "1 -0.616618 -0.850518 -1.105325  6.843760  5.836768  6.188684 -4.245044   \n",
      "2  2.435040 -1.703848  1.829917  7.447922  7.696975  3.554989  1.662780   \n",
      "\n",
      "        637       638       639       640       641        642        643  \\\n",
      "0 -3.719153  6.475679  5.151862 -1.591610  8.854741  10.064381   6.063772   \n",
      "1 -5.282600  4.913166  3.726776  0.924107  3.901019   6.103179  10.209585   \n",
      "2 -4.672365  5.256974  8.450525 -2.554128  6.850770  10.975472  10.417906   \n",
      "\n",
      "        644       645       646       647       648       649       650  \\\n",
      "0  9.098594 -2.071487  7.183352  2.985304 -0.422673 -2.882015 -0.281600   \n",
      "1  9.378163 -3.023943  3.360054 -0.938587 -5.264317  1.120414  1.323389   \n",
      "2  9.171949 -4.062702  0.595435  7.494442 -0.128083 -0.164977 -2.913484   \n",
      "\n",
      "        651       652       653       654       655        656        657  \\\n",
      "0  5.920300 -4.210787  6.930929  2.068095  0.318596   7.215537   3.966707   \n",
      "1  7.677933 -6.076982  9.024158 -0.234944  2.970109  11.622187  11.149906   \n",
      "2  5.908642 -9.312344  6.565113  2.094095  2.823307   9.490135   8.902680   \n",
      "\n",
      "        658       659       660       661       662       663       664  \\\n",
      "0  7.402778  5.087849  5.319235  3.481182  3.382199  1.785186  8.936787   \n",
      "1  0.085130  1.213213  5.017893  2.520942  5.026696  4.030759  7.808572   \n",
      "2  3.338731  3.342359  4.481447  1.429680  6.193045  7.314556  8.371704   \n",
      "\n",
      "        665       666       667       668       669       670       671  \\\n",
      "0  4.076917  1.483811 -0.562640 -1.483149  8.212433 -4.834332  4.153181   \n",
      "1  5.356614  0.184219  1.176217 -1.836503  3.160546 -1.851420  2.616456   \n",
      "2  4.921657  2.347979 -0.335475 -3.567768  9.385899 -3.509690  5.849824   \n",
      "\n",
      "        672       673       674       675       676       677        678  \\\n",
      "0 -4.727102  2.352012  2.372939 -3.085843  4.484458 -1.534073   8.517864   \n",
      "1 -3.463039  5.129566  2.922565 -5.166350  5.633355  1.458320   7.035646   \n",
      "2 -2.365581  4.515293 -1.397377 -1.201148  4.508380 -1.613801  14.121421   \n",
      "\n",
      "        679       680       681       682       683       684       685  \\\n",
      "0  0.459862 -0.679384  1.729537 -1.757404  2.014105  8.619559  1.731950   \n",
      "1  0.090134  4.114582 -1.578290 -3.253333 -1.569996  9.545050 -2.010516   \n",
      "2  1.948671  0.216062  1.643270 -2.285205 -0.146471  3.985831 -0.322800   \n",
      "\n",
      "        686       687       688       689       690       691        692  \\\n",
      "0  4.783137  2.458929  2.911312  1.628047 -4.258333 -3.082326  -9.189903   \n",
      "1  4.414790  0.196548  7.089420 -0.231884  3.369543 -2.723844 -13.454144   \n",
      "2  3.442761  2.361595  4.409281 -0.565290 -0.342249 -2.509265  -5.416868   \n",
      "\n",
      "        693       694       695       696       697       698       699  \\\n",
      "0  6.855521  3.572476  0.914572  2.878568  0.626511  1.306734  2.993352   \n",
      "1  8.130097  6.550124 -2.676625  0.022996  0.874508 -3.710469  2.407156   \n",
      "2  6.673698  7.431663  1.490439  1.067545 -0.951312  0.478861  6.375668   \n",
      "\n",
      "        700       701       702       703       704       705       706  \\\n",
      "0 -9.549967  0.049683  0.138965 -0.415130 -0.361673  4.592027 -0.726750   \n",
      "1 -8.580431  4.532781 -2.036993  1.694875  1.656891  7.844810  5.502418   \n",
      "2 -7.158092 -1.283926 -3.142252  0.310149  3.116590  8.188635  0.875539   \n",
      "\n",
      "        707       708       709       710        711       712        713  \\\n",
      "0  3.649524 -1.500627 -2.203618  0.481776   6.177304  4.745919  14.932029   \n",
      "1  5.062726  1.292654  3.929867  0.272810   4.026951  9.224670   8.824539   \n",
      "2  0.815470  0.834167  0.636117  1.727614  10.422323  6.946987  12.591754   \n",
      "\n",
      "        714       715       716       717       718       719       720  \\\n",
      "0 -2.740313 -5.774817  2.278373  0.378722  9.911015 -0.381399  2.536070   \n",
      "1 -0.579542 -1.089729 -0.164584 -1.742343  8.330041 -3.782599  2.018847   \n",
      "2 -5.132142 -4.723969  1.197233 -3.376830  9.730207  0.204729  2.761517   \n",
      "\n",
      "        721       722       723       724       725       726       727  \\\n",
      "0  6.391813 -8.773495  0.753500  4.267003 -1.387893 -1.487998 -2.040565   \n",
      "1  9.127346 -8.617350  0.938348  4.883005  3.756557 -1.796624 -5.075623   \n",
      "2  7.984827 -6.420674  0.962958  5.931072 -0.043517 -0.879816 -4.434582   \n",
      "\n",
      "         728       729       730       731       732       733        734  \\\n",
      "0  -7.306420  5.983499 -3.070429  5.680333  0.379768 -1.462666  11.452141   \n",
      "1 -10.609557  5.170793  0.236461  4.561769 -6.386197 -1.565668  12.180112   \n",
      "2 -10.029642  2.435761  1.016757  5.507775 -2.049133 -3.707269  13.600769   \n",
      "\n",
      "        735        736       737       738       739       740       741  \\\n",
      "0  4.760187  10.576671 -6.464797 -1.712859 -0.784617  3.620527 -3.665282   \n",
      "1  0.259595   8.725155 -5.142482  0.406938 -5.080133  0.102261 -3.464976   \n",
      "2  6.717438   8.551674 -1.318241 -1.514848 -1.493267  0.343194 -1.768174   \n",
      "\n",
      "        742       743        744       745       746       747       748  \\\n",
      "0  3.133387  4.727549   6.337965  5.137142 -0.767282  2.052627  8.055152   \n",
      "1  5.185787  2.392219   8.177595  2.558066 -1.710488  4.609308  2.650991   \n",
      "2  3.253260  2.905824  10.466574  1.111540 -1.781683  3.922219  4.676786   \n",
      "\n",
      "        749        750       751       752       753       754       755  \\\n",
      "0  1.181919   9.449754  1.138614 -0.555371 -0.849948  1.242219  5.273076   \n",
      "1  4.893968  10.448481  1.310930  1.152630 -0.492094  1.807053  4.207174   \n",
      "2  1.524059   5.452254 -2.274525  1.606924  0.081997  4.283435  8.715678   \n",
      "\n",
      "        756       757       758       759       760       761       762  \\\n",
      "0 -5.270195 -0.517817  1.591425  4.450572 -1.541360  1.614022 -0.792689   \n",
      "1 -1.700002  2.112107 -0.098373  5.196072  5.116686  0.084854  3.427706   \n",
      "2 -1.463426 -0.613912  3.057417  2.265095  3.859926 -0.582457  2.405440   \n",
      "\n",
      "        763       764       765       766       767       768       769  \\\n",
      "0  2.675606  0.981988  2.990088  3.717855  3.653548 -1.408289 -2.107412   \n",
      "1  4.723933 -2.888217  5.747456  2.852472  6.531560 -2.614019 -4.433739   \n",
      "2  2.364013  2.831348  3.965599  0.983998  2.859712 -0.783027 -0.877815   \n",
      "\n",
      "        770       771       772       773       774       775       776  \\\n",
      "0  2.007258  3.242882 -0.381848  0.191489  3.095776  5.433972  0.844188   \n",
      "1  1.384849  2.604265  0.283964 -1.405588  2.418120  6.147124  0.820276   \n",
      "2 -1.486745  3.445007 -2.643519 -2.325731  4.366029  3.044344  4.511201   \n",
      "\n",
      "        777       778       779       780       781       782       783  \\\n",
      "0 -0.592530  0.622219  0.503834  0.268034  6.158455  5.232590 -3.474305   \n",
      "1  0.666043  0.451174  0.120613 -0.191020  7.171941  4.123363 -4.377471   \n",
      "2  0.182997  1.465412 -0.862017  1.863472  9.873694  6.632189 -2.755573   \n",
      "\n",
      "        784       785       786       787       788       789       790  \\\n",
      "0  2.289078  1.253137  0.557547  7.505813  2.967334  8.647754  1.852747   \n",
      "1  6.653153  1.435608 -1.464581  6.110433 -2.278414  7.762423  2.750364   \n",
      "2  3.382203  2.535740 -0.849461  9.285320  1.678334  3.602233  1.200676   \n",
      "\n",
      "        791       792       793       794       795       796       797  \\\n",
      "0  0.991235 -3.076854 -0.206251 -0.937376  7.500220  9.224415  7.317262   \n",
      "1  0.152283 -3.687150 -1.170113 -3.919014  5.898480  8.521063  3.276588   \n",
      "2  3.077588 -5.490947  0.165744 -0.966461  7.350812  5.317441  3.729595   \n",
      "\n",
      "        798       799       800       801       802       803       804  \\\n",
      "0  4.648260  0.974563  7.994916  2.103754  4.233702  0.381443  0.392755   \n",
      "1  2.663350  2.119076  4.746778  3.368060  5.047732 -1.120533 -1.863871   \n",
      "2  2.555493  5.042006  9.038880  2.183992  3.702011 -1.332662 -0.094933   \n",
      "\n",
      "        805       806       807       808       809       810       811  \\\n",
      "0  0.472851 -3.542834  5.301157 -1.894004  4.017774  1.891438 -0.230567   \n",
      "1 -1.566990 -5.363639  3.213177  1.968181  5.332388  2.506620 -1.130086   \n",
      "2 -1.593725 -6.072054  4.162358 -3.400574  6.281246  4.127687 -2.116608   \n",
      "\n",
      "        812       813        814       815       816       817       818  \\\n",
      "0 -8.217907 -0.423519  -8.643490 -1.174694 -3.874310  2.247958 -5.063566   \n",
      "1 -9.048694  4.435231  -2.711926 -1.929049 -3.461081  3.120049 -8.194336   \n",
      "2 -4.471682 -1.538123 -10.799488  1.157623 -2.000542 -0.171762 -4.783696   \n",
      "\n",
      "        819       820       821       822       823       824       825  \\\n",
      "0  0.060418  2.815841  4.190797  5.180801  2.037723  1.942374 -0.613576   \n",
      "1  1.766794 -0.745019  4.690620  0.572102  1.642499  4.143167 -1.574016   \n",
      "2  5.840811  0.340301  5.186409  6.602096  1.381015  3.076777 -2.639323   \n",
      "\n",
      "        826       827       828       829       830       831       832  \\\n",
      "0  1.934769  2.332740  1.909075 -2.354058 -0.680895  0.675981 -1.854009   \n",
      "1 -3.985442  3.752186  2.496957 -2.249908 -6.376089  2.501241 -2.048720   \n",
      "2  8.132322  2.369693  1.206556 -4.867464 -2.348637  3.009338 -1.816291   \n",
      "\n",
      "         833       834       835        836       837       838       839  \\\n",
      "0   7.942177  1.162664 -0.665433   7.067408  6.608552  1.786023  0.467862   \n",
      "1  10.156159  2.616576 -0.469858  13.844138  6.864612  5.507941  1.801562   \n",
      "2  10.405237  3.006481  0.265475  13.587607  8.477172  0.274770  4.933721   \n",
      "\n",
      "        840       841       842       843        844        845        846  \\\n",
      "0  1.920928 -3.598096 -3.761979 -1.655360  11.795938  -5.843216   3.233383   \n",
      "1  0.673302 -5.170696  0.285527 -0.088925  14.225945  -8.127195  10.050869   \n",
      "2  1.538135 -4.231970 -2.677162 -0.784544  13.590789 -10.107424   7.388308   \n",
      "\n",
      "        847       848       849       850       851       852       853  \\\n",
      "0  5.962056  4.117456  0.684645 -2.804895  4.265742  1.716675  1.874140   \n",
      "1  4.236279  5.228606  4.069227  0.970257  6.057297  8.692668  4.986397   \n",
      "2  4.633121 -0.028238  1.030336 -4.037041  5.369930  2.190950  3.601803   \n",
      "\n",
      "        854       855       856       857       858       859        860  \\\n",
      "0 -0.507770 -3.685929 -0.713212 -3.033026  3.628868 -6.923140  10.932699   \n",
      "1  7.848945 -3.269419  1.326179 -5.098634  2.429621 -4.547063   4.916811   \n",
      "2  3.312404 -1.681676  2.410336 -3.699524  5.306048 -3.602980  14.273178   \n",
      "\n",
      "        861       862       863       864       865       866       867  \\\n",
      "0  3.786296 -5.212471  1.246474  6.746320  9.495256  5.114130  2.547456   \n",
      "1  7.231016 -2.416590  2.863978  1.970501  8.934120  0.213180  4.516782   \n",
      "2  5.427159 -6.382572  1.091380  9.234424  6.022137  7.617663  0.946178   \n",
      "\n",
      "        868       869       870       871       872       873       874  \\\n",
      "0  0.547748  3.702599  3.447077  0.554405 -6.635043  5.167167 -5.571281   \n",
      "1  3.750843  5.099149 -1.248072 -0.889726 -3.253022  4.617873 -7.821917   \n",
      "2  1.039847  8.237604  3.094096 -0.528181 -6.963658  5.236725 -6.380414   \n",
      "\n",
      "         875       876       877       878       879       880       881  \\\n",
      "0   6.601067  1.550506  0.987959  5.120656 -2.499764 -0.949691  0.406625   \n",
      "1  11.864473  0.188282  4.026437  5.469885 -1.622287  2.567145 -2.642861   \n",
      "2   7.514213  3.008770  3.457739  4.216575 -6.472163  0.785790 -7.662918   \n",
      "\n",
      "        882       883       884       885       886       887       888  \\\n",
      "0  3.841552  3.434896  2.578423 -2.985136  0.516458  2.845689  4.766621   \n",
      "1  3.785675  0.339179  1.284318 -0.467665 -0.488705  8.182800  0.985394   \n",
      "2  2.936180  7.187555  1.867444 -1.846847  1.816632  5.905972  5.138014   \n",
      "\n",
      "        889       890       891       892       893       894       895  \\\n",
      "0  6.240398  5.910903 -7.009205  0.336686  3.032632  3.008120 -1.077397   \n",
      "1  8.134197  7.024554 -9.901287  2.791431  6.976445  3.230097  0.425819   \n",
      "2  7.544888  7.065956 -9.907246 -4.298956  8.100222  5.028630  2.393779   \n",
      "\n",
      "        896       897       898       899       900       901       902  \\\n",
      "0 -2.122043 -2.824288  0.607031 -0.702424  5.600156  1.065695  2.965724   \n",
      "1 -0.342620 -5.619911  0.310682 -3.502754  5.464519  2.227177  3.658025   \n",
      "2 -0.543977 -1.765779  2.406233 -6.327339  3.500044  1.125968  3.561208   \n",
      "\n",
      "        903       904       905       906       907        908       909  \\\n",
      "0  9.575561  0.188325  2.492651 -3.862015  0.566317  10.778276 -0.509438   \n",
      "1  8.256838  4.219907  3.273704 -4.035900 -1.048617   7.768247  6.794518   \n",
      "2  7.476434  1.739857  6.273011 -1.641907  2.649104  12.766959  2.227485   \n",
      "\n",
      "        910       911       912       913       914       915       916  \\\n",
      "0  5.434574  6.044135 -0.171998 -3.422001  4.869468 -0.473108  3.936992   \n",
      "1  3.987674  6.584672 -1.355059 -1.561361  4.608219 -2.422895  2.891892   \n",
      "2  5.624333  7.497246  0.364111 -4.400733  5.314129  1.577184  4.960024   \n",
      "\n",
      "        917       918       919       920       921       922       923  \\\n",
      "0  4.980178  2.779492  4.312110  2.088060 -4.805147 -0.748880  1.382757   \n",
      "1  9.260674  1.820140  2.551911  4.527046 -3.991904  5.220912 -4.129735   \n",
      "2  5.341125  4.192294  3.978727  3.624759 -7.203017  2.264046 -2.318784   \n",
      "\n",
      "        924        925       926       927       928       929        930  \\\n",
      "0  5.251216   9.429535 -0.690414  2.276350  3.167572 -0.021344  -6.982346   \n",
      "1 -0.536602  11.843204  0.122986  1.527112  2.778838  3.486353  -4.298670   \n",
      "2  1.195675  12.961411 -2.420590 -0.312071  3.453766  4.301141 -11.509254   \n",
      "\n",
      "        931       932       933       934       935       936       937  \\\n",
      "0 -1.246387  2.585068  5.739308 -3.683774  1.186197  0.636276 -2.399755   \n",
      "1 -2.977555  1.892730 -2.008558 -3.753169  2.226268 -1.685282 -2.115381   \n",
      "2 -1.413722  7.596390  2.211997 -3.769562  5.576261 -3.265179 -2.662830   \n",
      "\n",
      "        938       939       940       941       942       943       944  \\\n",
      "0  1.570134  1.092569 -4.729311 -4.211257  5.598343  4.479137  2.626797   \n",
      "1  3.449778 -4.056810 -0.018867 -3.665839  8.810637  5.692514 -1.786497   \n",
      "2  4.620731 -0.574317 -5.237612 -5.052721  5.835349  1.010785  0.790517   \n",
      "\n",
      "        945       946       947       948        949       950       951  \\\n",
      "0 -1.530554 -3.288750  1.000692  7.683235   6.705755  4.525030  0.688076   \n",
      "1 -7.312487 -2.485815  5.099624  5.939199   7.603899  3.349198  4.175889   \n",
      "2 -1.838253  2.212535  4.302318  5.315802  10.674754  3.346724  4.939938   \n",
      "\n",
      "         952       953       954       955       956       957        958  \\\n",
      "0  11.205934  3.711983 -6.340749  6.075386  2.047981 -4.959070  -7.444678   \n",
      "1   7.753559  5.481102 -4.153688  5.041199  3.487773 -6.573977  -8.650679   \n",
      "2   5.216081  0.731112 -7.303160  4.307053  4.366264 -6.234616 -13.152252   \n",
      "\n",
      "         959       960       961       962       963       964       965  \\\n",
      "0  20.141972  6.833852  6.083749  1.131049  2.323809  1.612696  4.739998   \n",
      "1  18.824377  4.821194  5.355461  2.403030  6.119947  3.336806  6.670469   \n",
      "2  16.480083  5.687193  4.791486  3.431655  4.362817  0.976956  6.731501   \n",
      "\n",
      "        966       967       968       969       970       971       972  \\\n",
      "0  4.591519 -3.354251  3.864141 -0.598768  5.118748  1.077781 -5.926867   \n",
      "1  3.455216  2.554153  3.514037  0.027322  4.298616  3.283406 -5.762742   \n",
      "2  7.417157 -2.135030  6.565089 -0.360006  3.051651 -3.065247 -6.116168   \n",
      "\n",
      "        973       974       975       976       977       978       979  \\\n",
      "0  5.604502 -3.119830  7.112538 -8.976082  1.065463 -0.534651 -2.806047   \n",
      "1  8.030577  1.699280  6.551629 -2.246417  1.915627  2.334428 -1.238937   \n",
      "2  2.470692  0.141349  6.678517 -6.401095  0.643439  2.416507 -3.078562   \n",
      "\n",
      "        980       981       982       983       984       985       986  \\\n",
      "0  1.302769  8.062064  2.822860  3.943606  0.324094 -3.928993 -5.053246   \n",
      "1 -0.271375  6.445001  2.585915 -1.750560 -3.860883 -7.428352 -6.577626   \n",
      "2  0.960048  9.902675  2.054840  3.256564 -3.247262 -6.383999 -7.141603   \n",
      "\n",
      "        987       988       989       990       991       992       993  \\\n",
      "0  1.823744 -1.056852 -6.064230  0.831500  1.058575 -2.159902  6.502488   \n",
      "1 -0.746363 -2.930600 -5.308636  1.391882 -1.446613 -3.586621  3.637438   \n",
      "2  1.080929 -3.813707 -4.054895  4.890376 -1.513555  0.150391  8.686028   \n",
      "\n",
      "        994       995       996       997       998       999      1000  \\\n",
      "0  2.183602  0.808089  1.246657  5.693762  4.810176  7.745796  1.917970   \n",
      "1  4.136292  0.260804 -1.366696  4.857075  6.324381  8.317616  1.121613   \n",
      "2  4.989118 -0.527386 -0.264544  3.233610  5.212184  8.502142  2.773831   \n",
      "\n",
      "       1001      1002      1003      1004      1005      1006       1007  \\\n",
      "0  1.683999  2.459538 -0.818448 -7.790128  1.319279 -7.441175   5.276613   \n",
      "1  2.661363  0.432163 -2.423065 -7.906262  0.091893 -6.191575   5.611105   \n",
      "2 -0.094005  0.796390 -1.269817 -4.600290  2.017950 -4.941922  10.482992   \n",
      "\n",
      "       1008      1009      1010      1011      1012      1013      1014  \\\n",
      "0  2.965866 -8.285309  1.198300  1.602538 -0.587342  0.429838  5.647770   \n",
      "1  6.008004 -8.573439 -3.238862  3.061680  2.204786  1.545850  7.702147   \n",
      "2  6.270562 -8.612421 -0.266095  1.524185 -6.157193  1.232113  6.434234   \n",
      "\n",
      "       1015      1016      1017      1018      1019      1020      1021  \\\n",
      "0  8.721684 -0.333245  1.502728 -1.612393  6.950094 -0.062648  3.052343   \n",
      "1  6.678703  0.590560  4.745173 -7.257598  3.307536  1.857881  4.014536   \n",
      "2  9.191558 -2.789804  9.759512  0.615079  6.004198  1.696148  7.325929   \n",
      "\n",
      "       1022      1023  \n",
      "0  3.045043  8.627454  \n",
      "1  3.806489  4.361258  \n",
      "2  3.691381  9.276790  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1567/1567 [1:14:29<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 7026.62 MiB, increment: 3910.98 MiB\n"
     ]
    }
   ],
   "source": [
    "# Create the test dataset and dataloader\n",
    "batch_size = 2 ** 8\n",
    "\n",
    "if OWN_INSTANCE:\n",
    "    batch_size = 2 ** 8\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "model = None\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = AutoModelForImageClassification.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "\n",
    "def combine_vit_features(dataloader, metadata):\n",
    "    NUM_FEATURES = 1024\n",
    "    feat_cols = list(range(NUM_FEATURES))\n",
    "\n",
    "    table = metadata.values\n",
    "    table = np.hstack((table, np.zeros((table.shape[0], NUM_FEATURES + 1))))\n",
    "    columns = metadata.columns.values.tolist()\n",
    "    columns.append('vitmae_target')\n",
    "    columns.extend(feat_cols)\n",
    "\n",
    "    row_offset = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, batch_ids in tqdm(dataloader, total = len(dataloader)):\n",
    "            inputs = inputs.to(device)\n",
    "            # print(inputs.element_size() * inputs.nelement())\n",
    "            outputs = model(inputs, output_hidden_states = True)\n",
    "\n",
    "            proba = outputs.logits.cpu()\n",
    "            proba = softmax(proba, axis=1)[:, 1]\n",
    "\n",
    "            last_hidden_states = outputs.hidden_states[-1].cpu()\n",
    "            # print(last_hidden_states.shape)\n",
    "            cls_features = last_hidden_states[:, 0, :].squeeze().numpy().tolist()\n",
    "\n",
    "            for i, isic_id in enumerate(batch_ids):\n",
    "                row_idx = row_offset + i\n",
    "                table[row_idx, -NUM_FEATURES - 1] = proba[i]\n",
    "                table[row_idx, -NUM_FEATURES:] = cls_features[i]\n",
    "\n",
    "            row_offset += len(batch_ids)\n",
    "\n",
    "    # metadata.reset_index(inplace = True)\n",
    "    return pd.DataFrame(table, columns = columns)\n",
    "\n",
    "%memit test = combine_vit_features(test_dataloader, test)\n",
    "print(test)\n",
    "%memit train = combine_vit_features(train_dataloader, train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('target',axis=1)\n",
    "y = train['target']\n",
    "\n",
    "def pauc_above_tpr(solution, submission, min_tpr: float=0.80):\n",
    "    v_gt = abs(np.asarray(solution)-1)\n",
    "    v_pred = np.array([1.0 - x for x in submission])\n",
    "    max_fpr = abs(1-min_tpr)\n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    return partial_auc\n",
    "\n",
    "def Train_ML(model_factory, X, y, test_data):\n",
    "    # k-fold cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    test_predictions = [] \n",
    "    models = []\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(tqdm(skf.split(X, y), total=n_splits), 1):\n",
    "        # StratifiedKFold yields the indices from which we retrieve pandas metadata\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_val = y[train_index], y[test_index]\n",
    "        \n",
    "        model = model_factory()\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # record performance on all sets\n",
    "        y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "        train_pauc = pauc_above_tpr(y_train,y_train_pred_proba, min_tpr=0.8)\n",
    "        train_scores.append(train_pauc)\n",
    "\n",
    "        y_val_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "        val_pauc = pauc_above_tpr(y_val, y_val_pred_proba, min_tpr=0.8)\n",
    "        val_scores.append(val_pauc)\n",
    "        \n",
    "        # make predictions\n",
    "        y_test_pred_proba = model.predict_proba(test)[:, 1]\n",
    "        test_predictions.append(y_test_pred_proba)\n",
    "        \n",
    "        models.append(model)\n",
    "\n",
    "        print(f\"Fold {fold}: Train pAUC = {train_pauc:.4f}, Validation pAUC = {val_pauc:.4f}\")\n",
    "\n",
    "    # mean pauc on different folds' models\n",
    "    mean_train_pauc = np.mean(train_scores)\n",
    "    mean_val_pauc = np.mean(val_scores)\n",
    "\n",
    "    print(f\"\\nMean Train pAUC: {mean_train_pauc:.4f}\")\n",
    "    print(f\"Mean Validation pAUC: {mean_val_pauc:.4f}\")\n",
    "\n",
    "    # why would you want the \"model\"?\n",
    "    return model,test_predictions, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████████████████▋                                                                                 | 1/3 [01:37<03:14, 97.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Train pAUC = 0.2000, Validation pAUC = 0.1845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|█████████████████████████████████████████████████████████████████████████████████▎                                        | 2/3 [03:13<01:36, 96.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: Train pAUC = 0.2000, Validation pAUC = 0.1734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [04:50<00:00, 96.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3: Train pAUC = 0.2000, Validation pAUC = 0.1875\n",
      "\n",
      "Mean Train pAUC: 0.2000\n",
      "Mean Validation pAUC: 0.1818\n",
      "CPU times: user 1h 7min 59s, sys: 12 s, total: 1h 8min 12s\n",
      "Wall time: 4min 50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def lgbm_factory():\n",
    "    params =  {\n",
    "            'objective': 'binary', 'colsample_bytree': 0.6852015051268027, 'max_depth': 4, \n",
    "            'learning_rate': 0.05714390301637632, 'n_estimators': 1010, 'subsample': 0.13326633837138008, \n",
    "            'lambda_l1': 1.4445754309498806e-08, 'lambda_l2': 0.11031259304642657, 'boosting_type': 'dart'\n",
    "                }\n",
    "    \n",
    "    Model = LGBMClassifier(**params,verbose=-1,random_state=SEED,\n",
    "                          extra_tree=True,max_bin=250,reg_alpha=0.1,reg_lambda=0.8\n",
    "                          )\n",
    "    return Model\n",
    "\n",
    "train_lgb, test_preds, all_models = Train_ML(lgbm_factory, X, y, test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████████████████▎                                                                                | 1/3 [02:23<04:46, 143.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Train pAUC = 0.1973, Validation pAUC = 0.1852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████████████████████████████████████████████████████████▋                                        | 2/3 [04:44<02:21, 141.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: Train pAUC = 0.1980, Validation pAUC = 0.1771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [07:20<00:00, 146.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3: Train pAUC = 0.1966, Validation pAUC = 0.1889\n",
      "\n",
      "Mean Train pAUC: 0.1973\n",
      "Mean Validation pAUC: 0.1837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 21624.50 MiB, increment: 5934.84 MiB\n",
      "CPU times: user 46min 14s, sys: 26min 58s, total: 1h 13min 12s\n",
      "Wall time: 7min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def cat_factory():\n",
    "    Cat_Model = CatBoostClassifier(verbose=0,random_state=SEED,\n",
    "                              iterations = 1000,\n",
    "                              learning_rate=0.01,\n",
    "                              objective = 'Logloss',\n",
    "                              boosting_type = 'Plain',\n",
    "                              bootstrap_type = 'Bernoulli',\n",
    "                              colsample_bylevel = 0.08656159895289164,\n",
    "                              subsample = 0.46623542352578917,\n",
    "                              depth=9,)\n",
    "    return Cat_Model\n",
    "\n",
    "%memit train_cat, cat_test_preds , Cat_all_models = Train_ML(cat_factory, X, y, test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████████████████▎                                                                                | 1/3 [02:59<05:58, 179.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Train pAUC = 0.2000, Validation pAUC = 0.1849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████████████████████████████████████████████████████████▋                                        | 2/3 [05:34<02:45, 165.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: Train pAUC = 0.2000, Validation pAUC = 0.1743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [08:02<00:00, 160.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3: Train pAUC = 0.2000, Validation pAUC = 0.1884\n",
      "\n",
      "Mean Train pAUC: 0.2000\n",
      "Mean Validation pAUC: 0.1825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 25977.86 MiB, increment: 8641.46 MiB\n",
      "CPU times: user 1h 46min 31s, sys: 21.1 s, total: 1h 46min 52s\n",
      "Wall time: 8min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def xgb_factory():\n",
    "    xgb_params2 = {\n",
    "        'objective': 'binary:logistic', 'colsample_bytree': 0.11756728710020253,'max_depth': 4, \n",
    "        'learning_rate': 0.009393224320850784,'n_estimators': 1227, 'subsample': 0.9589462514195692,\n",
    "        'lambda': 0.34216652262461505,'alpha': 1.150597512455824e-07\n",
    "                  }\n",
    "    \n",
    "    xgb_Model = XGBClassifier(**xgb_params2,random_state=SEED)\n",
    "    return xgb_Model\n",
    "\n",
    "%memit train_xgb, xgb_test_preds , xgb_all_models = Train_ML(xgb_factory, X, y, test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.21 ms, sys: 226 μs, total: 6.44 ms\n",
      "Wall time: 5.22 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isic_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0015657</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0015729</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0015740</td>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        isic_id    target\n",
       "0  ISIC_0015657  0.000017\n",
       "1  ISIC_0015729  0.000006\n",
       "2  ISIC_0015740  0.000023"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sample_file = '/kaggle/input/isic-2024-challenge/sample_submission.csv'\n",
    "\n",
    "if OWN_INSTANCE:\n",
    "    sample_file = 'data/sample_submission.csv'\n",
    "    \n",
    "Sample = pd.read_csv(sample_file)\n",
    "\n",
    "lgb_test = np.mean(test_preds, axis=0)\n",
    "cat_test = np.mean(cat_test_preds, axis=0)\n",
    "xgb_test = np.mean(xgb_test_preds, axis=0)\n",
    "\n",
    "\n",
    "ensemble_preds = (lgb_test + cat_test + xgb_test) / 3\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    'isic_id': Sample['isic_id'],\n",
    "    'target': ensemble_preds\n",
    "})\n",
    "\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_models(framework, models):\n",
    "    for idx, model in enumerate(models):\n",
    "        joblib.dump(model, f'gradboost/{framework}_{idx}.joblib')\n",
    "\n",
    "dump_models(\"lgbm\", all_models)\n",
    "dump_models(\"catboost\", Cat_all_models)\n",
    "dump_models(\"xgb\", xgb_all_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9094797,
     "sourceId": 63056,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 112702,
     "modelInstanceId": 88477,
     "sourceId": 105577,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
