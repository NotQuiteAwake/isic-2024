{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation for gradient boosting + vit feature extraction\n",
    "\n",
    "Removed all modifications to the train set except those necessary for feature engineering.\n",
    "\n",
    "References:\n",
    "\n",
    "- https://www.kaggle.com/code/abdmental01/multimodel-isic\n",
    "- https://www.kaggle.com/code/nosherwantahir/notebookea3cca46ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "\n",
    "import io\n",
    "import warnings\n",
    "import gc\n",
    "import sys\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from transformers import AutoModelForImageClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import category_encoders as ce\n",
    "\n",
    "import joblib\n",
    "\n",
    "OWN_INSTANCE = True\n",
    "SEED = 42\n",
    "n_splits = 3\n",
    "\n",
    "os.makedirs('gradboost', exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns present in test but not in train: set()\n",
      "Columns present in train but not in test: {'iddx_full', 'mel_mitotic_index', 'iddx_4', 'lesion_id', 'iddx_5', 'tbp_lv_dnn_lesion_confidence', 'mel_thick_mm', 'iddx_2', 'target', 'iddx_3', 'iddx_1'}\n",
      "CPU times: user 2.42 s, sys: 306 ms, total: 2.72 s\n",
      "Wall time: 2.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "test_metadata_file = '/kaggle/input/isic-2024-challenge/test-metadata.csv'\n",
    "train_metadata_file = '/kaggle/input/isic-2024-challenge/train-metadata.csv'\n",
    "\n",
    "if OWN_INSTANCE:\n",
    "    test_metadata_file = 'data/test-metadata.csv'\n",
    "    train_metadata_file = 'data/train-metadata.csv'\n",
    "\n",
    "test = pd.read_csv(test_metadata_file)\n",
    "train = pd.read_csv(train_metadata_file)\n",
    "\n",
    "#train.drop('isic_id',axis=1,inplace=True)\n",
    "#test.drop('isic_id',axis=1,inplace=True)\n",
    "\n",
    "test_columns = set(test.columns)\n",
    "train_columns = set(train.columns)\n",
    "\n",
    "diff_test_train = test_columns - train_columns\n",
    "diff_train_test = train_columns - test_columns\n",
    "\n",
    "if not diff_test_train and not diff_train_test:\n",
    "    print(\"Both DataFrames have the same columns.\")\n",
    "else:\n",
    "    print(\"Columns present in test but not in train:\", diff_test_train)\n",
    "    print(\"Columns present in train but not in test:\", diff_train_test)\n",
    "\n",
    "train.drop(columns=['iddx_4', 'mel_mitotic_index', 'iddx_1', 'lesion_id', 'tbp_lv_dnn_lesion_confidence',\n",
    "                    'iddx_5', 'mel_thick_mm', 'iddx_2', 'iddx_full', 'iddx_3'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.7 s, sys: 443 ms, total: 2.14 s\n",
      "Wall time: 2.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def fe(df):\n",
    "    \n",
    "    # a sort of eccentricity\n",
    "    df[\"lesion_size_ratio\"]=df[\"tbp_lv_minorAxisMM\"]/df[\"clin_size_long_diam_mm\"]\n",
    "    # another dimensionless measure of eccentricity (think circle / square)\n",
    "    df[\"lesion_shape_index\"]=df[\"tbp_lv_areaMM2\"]/(df[\"tbp_lv_perimeterMM\"]**2)\n",
    "    # contrast between hue inside and outside\n",
    "    df[\"hue_contrast\"]= (df[\"tbp_lv_H\"]-df[\"tbp_lv_Hext\"]).abs()\n",
    "    # contrast between luminance inside and outside\n",
    "    df[\"luminance_contrast\"]= (df[\"tbp_lv_L\"]-df[\"tbp_lv_Lext\"]).abs()\n",
    "    # LAB is another color space similar to RGB. delta's are inside v. outside.\n",
    "    df[\"lesion_color_difference\"]=np.sqrt(df[\"tbp_lv_deltaA\"]**2+df[\"tbp_lv_deltaB\"]**2+df[\"tbp_lv_deltaL\"]**2)\n",
    "    # both metrics increase when asymmetry is higher and are on scale 0-10\n",
    "    df[\"border_complexity\"]=df[\"tbp_lv_norm_border\"]+df[\"tbp_lv_symm_2axis\"]\n",
    "    # position on 3D TBP\n",
    "    df[\"3d_position_distance\"]=np.sqrt(df[\"tbp_lv_x\"]**2+df[\"tbp_lv_y\"]**2+df[\"tbp_lv_z\"]**2)\n",
    "    # another measure of irregularity...?\n",
    "    df[\"perimeter_to_area_ratio\"]=df[\"tbp_lv_perimeterMM\"]/df[\"tbp_lv_areaMM2\"]\n",
    "    # contrast between lesion and surrounding, values from 5-25 + color variation 0 - 10\n",
    "    df[\"lesion_visibility_score\"]=df[\"tbp_lv_deltaLBnorm\"]+df[\"tbp_lv_norm_color\"]\n",
    "    # both are location indicators\n",
    "    df[\"combined_anatomical_site\"]=df[\"anatom_site_general\"]+\"_\"+df[\"tbp_lv_location\"]\n",
    "    # only when both are large does a lesion score high on this (cf border_complexity)\n",
    "    df[\"symmetry_border_consistency\"]=df[\"tbp_lv_symm_2axis\"]*df[\"tbp_lv_norm_border\"]\n",
    "    # whether the variation in color is similar inside and outside lesion\n",
    "    df[\"color_consistency\"]=df[\"tbp_lv_stdL\"]/df[\"tbp_lv_stdLExt\"]\n",
    "    # interactions are just products\n",
    "    df[\"size_age_interaction\"]=df[\"clin_size_long_diam_mm\"]*df[\"age_approx\"]\n",
    "    # hue inside and color irregularity\n",
    "    df[\"hue_color_std_interaction\"]=df[\"tbp_lv_H\"]*df[\"tbp_lv_color_std_mean\"]\n",
    "    # three measures of irregularity combined.\n",
    "    df[\"lesion_severity_index\"]=(df[\"tbp_lv_norm_border\"]+df[\"tbp_lv_norm_color\"]+df[\"tbp_lv_eccentricity\"])/3\n",
    "    df[\"shape_complexity_index\"]=df[\"border_complexity\"]+df[\"lesion_shape_index\"]\n",
    "    # first three terms are average contrast, last term is contrast in immediately surrounding skin\n",
    "    df[\"color_contrast_index\"]=df[\"tbp_lv_deltaA\"]+df[\"tbp_lv_deltaB\"]+df[\"tbp_lv_deltaL\"]+df[\"tbp_lv_deltaLBnorm\"]\n",
    "    # the malignant lesions can be way longer and a log scale might better capture this\n",
    "    df[\"log_lesion_area\"]=np.log(df[\"tbp_lv_areaMM2\"]+1)\n",
    "    # perhaps lesion gorws in size with age.\n",
    "    df[\"normalized_lesion_size\"]=df[\"clin_size_long_diam_mm\"]/df[\"age_approx\"]\n",
    "    # internal and external hue averaged\n",
    "    df[\"mean_hue_difference\"]=(df[\"tbp_lv_H\"]+df[\"tbp_lv_Hext\"])/2\n",
    "    # combining inner contrast assuming Gaussisna\n",
    "    df[\"std_dev_contrast\"]=np.sqrt((df[\"tbp_lv_deltaA\"]**2+df[\"tbp_lv_deltaB\"]**2+df[\"tbp_lv_deltaL\"]**2)/3)\n",
    "    # combine metrics of color and shape, both could be more irregular for malignant\n",
    "    df[\"color_shape_composite_index\"]=(df[\"tbp_lv_color_std_mean\"]+df[\"tbp_lv_area_perim_ratio\"]+df[\"tbp_lv_symm_2axis\"])/3\n",
    "    df[\"3d_lesion_orientation\"]=np.arctan2(df[\"tbp_lv_y\"],df[\"tbp_lv_x\"])\n",
    "    df[\"overall_color_difference\"]=(df[\"tbp_lv_deltaA\"]+df[\"tbp_lv_deltaB\"]+df[\"tbp_lv_deltaL\"])/3\n",
    "    df[\"symmetry_perimeter_interaction\"]=df[\"tbp_lv_symm_2axis\"]*df[\"tbp_lv_perimeterMM\"]\n",
    "    # the larger this value, the larger the \"irregularity\"\n",
    "    df[\"comprehensive_lesion_index\"]=(df[\"tbp_lv_area_perim_ratio\"]+df[\"tbp_lv_eccentricity\"]+df[\"tbp_lv_norm_color\"]+df[\"tbp_lv_symm_2axis\"])/4\n",
    "    \n",
    "    # categorical columns\n",
    "    n_cat = [\"combined_anatomical_site\"]\n",
    "    \n",
    "    return df, n_cat\n",
    "\n",
    "train, n_cat = fe(train)\n",
    "test, _ = fe(test)\n",
    "\n",
    "# columns with categories\n",
    "cat_cols = [\"sex\", \"tbp_tile_type\", \"tbp_lv_location\", \"tbp_lv_location_simple\",'patient_id',\n",
    "   'anatom_site_general','copyright_license','attribution','image_type'] + n_cat\n",
    "\n",
    "# drop columns only present in one set\n",
    "def align_columns(train, test):\n",
    "    common_cols = train.columns.intersection(test.columns)\n",
    "    train = train[common_cols]\n",
    "    test = test[common_cols]\n",
    "    return train, test\n",
    "\n",
    "# target will be removed by align_columns anyway, remove first and add back later.\n",
    "target = train['target']\n",
    "train_features = train.drop(columns=['target'], errors='ignore')\n",
    "\n",
    "train_features_aligned, test_features_aligned = align_columns(train_features, test)\n",
    "\n",
    "encoder = ce.OrdinalEncoder(cols=cat_cols, handle_unknown='ignore')\n",
    "train = encoder.fit_transform(train_features_aligned)\n",
    "# a second call to encoder.transform will apply the same statistics of fit_transform.\n",
    "test = encoder.transform(test_features_aligned)\n",
    "\n",
    "train['target'] = target\n",
    "\n",
    "train.drop(columns = ['isic_id'], inplace = True)\n",
    "test.drop(columns = ['isic_id'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ViT and extract feature from last hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1971.01it/s]\n"
     ]
    }
   ],
   "source": [
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_path = '/kaggle/input/vit/transformers/default/1/'\n",
    "hdf_test_path = '/kaggle/input/isic-2024-challenge/test-image.hdf5'\n",
    "hdf_train_path = '/kaggle/input/isic-2024-challenge/train-image.hdf5'\n",
    "\n",
    "if OWN_INSTANCE:\n",
    "    model_path = 'TobanDjan/vit'\n",
    "    hdf_test_path = 'data/test-image.hdf5'\n",
    "    hdf_train_path = 'data/train-image.hdf5'\n",
    "\n",
    "# Function to load images from encoded data\n",
    "def load_image_from_encoded_data(encoded_data):\n",
    "    image = Image.open(io.BytesIO(encoded_data))\n",
    "    return image.convert('RGB')\n",
    "\n",
    "# Define a custom Dataset for the HDF5 images\n",
    "class HDF5TestDataset(Dataset):\n",
    "    def __init__(self, image_data, ids, transform=None):\n",
    "        self.image_data = image_data\n",
    "        self.ids = ids\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_data = self.image_data[idx]\n",
    "        image = load_image_from_encoded_data(image_data)\n",
    "        #imshow(image)\n",
    "        #plt.show()\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # print(image.element_size() * image.nelement())\n",
    "        # 602112 B = 0.574 MB\n",
    "        return image, self.ids[idx]\n",
    "\n",
    "def get_dataset(hdf_file_path):\n",
    "    with h5py.File(hdf_file_path, 'r') as f:\n",
    "        image_data = [f[image_id][()] for image_id in tqdm(f.keys())]\n",
    "        ids = list(f.keys())\n",
    "        dataset = HDF5TestDataset(image_data=image_data, ids=ids, transform=val_transform)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# %memit train_dataset = get_dataset(hdf_train_path)\n",
    "test_dataset = get_dataset(hdf_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1671.04 MiB, increment: 73.95 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the test dataset and dataloader\n",
    "batch_size = 2 ** 8\n",
    "\n",
    "if OWN_INSTANCE:\n",
    "    batch_size = 2 ** 8\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "model = None\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = AutoModelForImageClassification.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "\n",
    "def combine_vit_features(dataloader, metadata):\n",
    "    NUM_FEATURES = 768\n",
    "    feat_cols = list(range(NUM_FEATURES))\n",
    "\n",
    "    table = metadata.values\n",
    "    table = np.hstack((table, np.zeros((table.shape[0], NUM_FEATURES + 1))))\n",
    "    columns = metadata.columns.values.tolist()\n",
    "    columns.append('vit_target')\n",
    "    columns.extend(feat_cols)\n",
    "\n",
    "    row_offset = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, batch_ids in tqdm(dataloader, total = len(dataloader)):\n",
    "            inputs = inputs.to(device)\n",
    "            # print(inputs.element_size() * inputs.nelement())\n",
    "            outputs = model(inputs, output_hidden_states = True)\n",
    "\n",
    "            proba = outputs.logits.cpu()\n",
    "            proba = softmax(proba, axis=1)[:, 1]\n",
    "\n",
    "            last_hidden_states = outputs.hidden_states[-1].cpu()\n",
    "            cls_features = last_hidden_states[:, 0, :].squeeze().numpy().tolist()\n",
    "\n",
    "            for i, isic_id in enumerate(batch_ids):\n",
    "                row_idx = row_offset + i\n",
    "                table[row_idx, -NUM_FEATURES - 1] = proba[i]\n",
    "                table[row_idx, -NUM_FEATURES:] = cls_features[i]\n",
    "\n",
    "            row_offset += len(batch_ids)\n",
    "\n",
    "    # metadata.reset_index(inplace = True)\n",
    "    return pd.DataFrame(table, columns = columns)\n",
    "\n",
    "%memit test = combine_vit_features(test_dataloader, test)\n",
    "# %memit train = combine_vit_features(train_dataloader, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pauc_above_tpr(solution, submission, min_tpr: float=0.80):\n",
    "    v_gt = abs(np.asarray(solution)-1)\n",
    "    v_pred = np.array([1.0 - x for x in submission])\n",
    "    max_fpr = abs(1-min_tpr)\n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    return partial_auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1725.26 MiB, increment: 5.60 MiB\n",
      "peak memory: 1728.86 MiB, increment: 3.65 MiB\n",
      "peak memory: 1731.18 MiB, increment: 2.31 MiB\n"
     ]
    }
   ],
   "source": [
    "def Predict_GB(models, test_data):\n",
    "    # k-fold cross-validation\n",
    "    test_predictions = []\n",
    "\n",
    "    for model in models:\n",
    "        y_test_pred_proba = model.predict_proba(test_data)[:, 1]\n",
    "        test_predictions.append(y_test_pred_proba)\n",
    "\n",
    "    return test_predictions\n",
    "\n",
    "# TODO: set directory for Kaggle\n",
    "def load_models(framework:str):\n",
    "    models = [] \n",
    "    for idx in range(3):\n",
    "        model_file = \"\"\n",
    "        if OWN_INSTANCE:\n",
    "            model_file = f\"gradboost/{framework}_{idx}.joblib\"\n",
    "        model = joblib.load(model_file)\n",
    "        models.append(model)\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "all_models = load_models(\"lgbm\")\n",
    "Cat_all_models = load_models(\"catboost\")\n",
    "xgb_all_models = load_models(\"xgb\")\n",
    "\n",
    "%memit test_preds = Predict_GB(all_models, test)\n",
    "%memit cat_test_preds = Predict_GB(Cat_all_models, test)\n",
    "%memit xgb_test_preds = Predict_GB(xgb_all_models, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.78 ms, sys: 0 ns, total: 5.78 ms\n",
      "Wall time: 4.8 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isic_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0015657</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0015729</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0015740</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        isic_id    target\n",
       "0  ISIC_0015657  0.000025\n",
       "1  ISIC_0015729  0.000012\n",
       "2  ISIC_0015740  0.000018"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sample_file = '/kaggle/input/isic-2024-challenge/sample_submission.csv'\n",
    "\n",
    "if OWN_INSTANCE:\n",
    "    sample_file = 'data/sample_submission.csv'\n",
    "    \n",
    "Sample = pd.read_csv(sample_file)\n",
    "\n",
    "lgb_test = np.mean(test_preds, axis=0)\n",
    "cat_test = np.mean(cat_test_preds, axis=0)\n",
    "xgb_test = np.mean(xgb_test_preds, axis=0)\n",
    "\n",
    "ensemble_preds = (lgb_test + cat_test + xgb_test) / 3\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    'isic_id': Sample['isic_id'],\n",
    "    'target': ensemble_preds\n",
    "})\n",
    "\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "sub.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9094797,
     "sourceId": 63056,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 112702,
     "modelInstanceId": 88477,
     "sourceId": 105577,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
