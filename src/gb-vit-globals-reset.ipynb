{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/abdmental01/multimodel-isic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "\n",
    "import io\n",
    "import warnings\n",
    "import gc\n",
    "import sys\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from transformers import AutoModelForImageClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import category_encoders as ce\n",
    "\n",
    "import joblib\n",
    "\n",
    "OWN_INSTANCE = True\n",
    "SEED = 42\n",
    "n_splits = 3\n",
    "\n",
    "os.makedirs('gradboost', exist_ok = True)\n",
    "\n",
    "if 'env_globals' not in globals():\n",
    "    env_globals = list(globals().keys())\n",
    "\n",
    "if 'vars_preserve' not in globals():\n",
    "    vars_preserve = {'reset_globals', 'vars_preserve', 'env_globals'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set disable key to not force garbage collect. Better for debugging.\n",
    "def reset_globals(keep:list[str] = [], disabled:bool = False):\n",
    "    if disabled:\n",
    "        return\n",
    "    \n",
    "    globals_ = globals()\n",
    "    keep.extend(env_globals + list(vars_preserve))\n",
    "    to_save = {v: globals_[v] for v in keep}\n",
    "    \n",
    "    del globals_\n",
    "    get_ipython().magic(\"reset -f\")\n",
    "    globals().update(to_save)\n",
    "    \n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns present in test but not in train: set()\n",
      "Columns present in train but not in test: {'iddx_3', 'lesion_id', 'tbp_lv_dnn_lesion_confidence', 'mel_thick_mm', 'iddx_4', 'target', 'iddx_full', 'iddx_2', 'iddx_1', 'iddx_5', 'mel_mitotic_index'}\n",
      "CPU times: user 2.93 s, sys: 356 ms, total: 3.28 s\n",
      "Wall time: 3.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "test_metadata_file = '/kaggle/input/isic-2024-challenge/test-metadata.csv'\n",
    "train_metadata_file = '/kaggle/input/isic-2024-challenge/train-metadata.csv'\n",
    "\n",
    "if OWN_INSTANCE:\n",
    "    test_metadata_file = 'data/test-metadata.csv'\n",
    "    train_metadata_file = 'data/train-metadata.csv'\n",
    "\n",
    "test = pd.read_csv(test_metadata_file)\n",
    "train = pd.read_csv(train_metadata_file)\n",
    "\n",
    "#train.drop('isic_id',axis=1,inplace=True)\n",
    "#test.drop('isic_id',axis=1,inplace=True)\n",
    "\n",
    "test_columns = set(test.columns)\n",
    "train_columns = set(train.columns)\n",
    "\n",
    "diff_test_train = test_columns - train_columns\n",
    "diff_train_test = train_columns - test_columns\n",
    "\n",
    "if not diff_test_train and not diff_train_test:\n",
    "    print(\"Both DataFrames have the same columns.\")\n",
    "else:\n",
    "    print(\"Columns present in test but not in train:\", diff_test_train)\n",
    "    print(\"Columns present in train but not in test:\", diff_train_test)\n",
    "\n",
    "train.drop(columns=['iddx_4', 'mel_mitotic_index', 'iddx_1', 'lesion_id', 'tbp_lv_dnn_lesion_confidence',\n",
    "                    'iddx_5', 'mel_thick_mm', 'iddx_2', 'iddx_full', 'iddx_3'],inplace=True)\n",
    "\n",
    "vars_preserve.update({'train', 'test'})\n",
    "reset_globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.14 s, sys: 489 ms, total: 2.63 s\n",
      "Wall time: 2.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def fe(df):\n",
    "    \n",
    "    # a sort of eccentricity\n",
    "    df[\"lesion_size_ratio\"]=df[\"tbp_lv_minorAxisMM\"]/df[\"clin_size_long_diam_mm\"]\n",
    "    # another dimensionless measure of eccentricity (think circle / square)\n",
    "    df[\"lesion_shape_index\"]=df[\"tbp_lv_areaMM2\"]/(df[\"tbp_lv_perimeterMM\"]**2)\n",
    "    # contrast between hue inside and outside\n",
    "    df[\"hue_contrast\"]= (df[\"tbp_lv_H\"]-df[\"tbp_lv_Hext\"]).abs()\n",
    "    # contrast between luminance inside and outside\n",
    "    df[\"luminance_contrast\"]= (df[\"tbp_lv_L\"]-df[\"tbp_lv_Lext\"]).abs()\n",
    "    # LAB is another color space similar to RGB. delta's are inside v. outside.\n",
    "    df[\"lesion_color_difference\"]=np.sqrt(df[\"tbp_lv_deltaA\"]**2+df[\"tbp_lv_deltaB\"]**2+df[\"tbp_lv_deltaL\"]**2)\n",
    "    # both metrics increase when asymmetry is higher and are on scale 0-10\n",
    "    df[\"border_complexity\"]=df[\"tbp_lv_norm_border\"]+df[\"tbp_lv_symm_2axis\"]\n",
    "    # position on 3D TBP\n",
    "    df[\"3d_position_distance\"]=np.sqrt(df[\"tbp_lv_x\"]**2+df[\"tbp_lv_y\"]**2+df[\"tbp_lv_z\"]**2)\n",
    "    # another measure of irregularity...?\n",
    "    df[\"perimeter_to_area_ratio\"]=df[\"tbp_lv_perimeterMM\"]/df[\"tbp_lv_areaMM2\"]\n",
    "    # contrast between lesion and surrounding, values from 5-25 + color variation 0 - 10\n",
    "    df[\"lesion_visibility_score\"]=df[\"tbp_lv_deltaLBnorm\"]+df[\"tbp_lv_norm_color\"]\n",
    "    # both are location indicators\n",
    "    df[\"combined_anatomical_site\"]=df[\"anatom_site_general\"]+\"_\"+df[\"tbp_lv_location\"]\n",
    "    # only when both are large does a lesion score high on this (cf border_complexity)\n",
    "    df[\"symmetry_border_consistency\"]=df[\"tbp_lv_symm_2axis\"]*df[\"tbp_lv_norm_border\"]\n",
    "    # whether the variation in color is similar inside and outside lesion\n",
    "    df[\"color_consistency\"]=df[\"tbp_lv_stdL\"]/df[\"tbp_lv_stdLExt\"]\n",
    "    # interactions are just products\n",
    "    df[\"size_age_interaction\"]=df[\"clin_size_long_diam_mm\"]*df[\"age_approx\"]\n",
    "    # hue inside and color irregularity\n",
    "    df[\"hue_color_std_interaction\"]=df[\"tbp_lv_H\"]*df[\"tbp_lv_color_std_mean\"]\n",
    "    # three measures of irregularity combined.\n",
    "    df[\"lesion_severity_index\"]=(df[\"tbp_lv_norm_border\"]+df[\"tbp_lv_norm_color\"]+df[\"tbp_lv_eccentricity\"])/3\n",
    "    df[\"shape_complexity_index\"]=df[\"border_complexity\"]+df[\"lesion_shape_index\"]\n",
    "    # first three terms are average contrast, last term is contrast in immediately surrounding skin\n",
    "    df[\"color_contrast_index\"]=df[\"tbp_lv_deltaA\"]+df[\"tbp_lv_deltaB\"]+df[\"tbp_lv_deltaL\"]+df[\"tbp_lv_deltaLBnorm\"]\n",
    "    # the malignant lesions can be way longer and a log scale might better capture this\n",
    "    df[\"log_lesion_area\"]=np.log(df[\"tbp_lv_areaMM2\"]+1)\n",
    "    # perhaps lesion gorws in size with age.\n",
    "    df[\"normalized_lesion_size\"]=df[\"clin_size_long_diam_mm\"]/df[\"age_approx\"]\n",
    "    # internal and external hue averaged\n",
    "    df[\"mean_hue_difference\"]=(df[\"tbp_lv_H\"]+df[\"tbp_lv_Hext\"])/2\n",
    "    # combining inner contrast assuming Gaussisna\n",
    "    df[\"std_dev_contrast\"]=np.sqrt((df[\"tbp_lv_deltaA\"]**2+df[\"tbp_lv_deltaB\"]**2+df[\"tbp_lv_deltaL\"]**2)/3)\n",
    "    # combine metrics of color and shape, both could be more irregular for malignant\n",
    "    df[\"color_shape_composite_index\"]=(df[\"tbp_lv_color_std_mean\"]+df[\"tbp_lv_area_perim_ratio\"]+df[\"tbp_lv_symm_2axis\"])/3\n",
    "    df[\"3d_lesion_orientation\"]=np.arctan2(df[\"tbp_lv_y\"],df[\"tbp_lv_x\"])\n",
    "    df[\"overall_color_difference\"]=(df[\"tbp_lv_deltaA\"]+df[\"tbp_lv_deltaB\"]+df[\"tbp_lv_deltaL\"])/3\n",
    "    df[\"symmetry_perimeter_interaction\"]=df[\"tbp_lv_symm_2axis\"]*df[\"tbp_lv_perimeterMM\"]\n",
    "    # the larger this value, the larger the \"irregularity\"\n",
    "    df[\"comprehensive_lesion_index\"]=(df[\"tbp_lv_area_perim_ratio\"]+df[\"tbp_lv_eccentricity\"]+df[\"tbp_lv_norm_color\"]+df[\"tbp_lv_symm_2axis\"])/4\n",
    "    \n",
    "    # categorical columns\n",
    "    n_cat = [\"combined_anatomical_site\"]\n",
    "    \n",
    "    return df, n_cat\n",
    "\n",
    "train, n_cat = fe(train)\n",
    "test, _ = fe(test)\n",
    "\n",
    "# columns with categories\n",
    "cat_cols = [\"sex\", \"tbp_tile_type\", \"tbp_lv_location\", \"tbp_lv_location_simple\",'patient_id',\n",
    "   'anatom_site_general','copyright_license','attribution','image_type'] + n_cat\n",
    "\n",
    "# drop columns only present in one set\n",
    "def align_columns(train, test):\n",
    "    common_cols = train.columns.intersection(test.columns)\n",
    "    train = train[common_cols]\n",
    "    test = test[common_cols]\n",
    "    return train, test\n",
    "\n",
    "# target will be removed by align_columns anyway, remove first and add back later.\n",
    "target = train['target']\n",
    "train_features = train.drop(columns=['target'], errors='ignore')\n",
    "\n",
    "train_features_aligned, test_features_aligned = align_columns(train_features, test)\n",
    "\n",
    "encoder = ce.OrdinalEncoder(cols=cat_cols, handle_unknown='ignore')\n",
    "train = encoder.fit_transform(train_features_aligned)\n",
    "# a second call to encoder.transform will apply the same statistics of fit_transform.\n",
    "test = encoder.transform(test_features_aligned)\n",
    "\n",
    "train.drop(columns=['isic_id'], inplace = True)\n",
    "test.drop(columns=['isic_id'], inplace = True)\n",
    "\n",
    "train['target'] = target\n",
    "\n",
    "reset_globals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ViT and extract feature from last hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 401059/401059 [00:54<00:00, 7386.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2418.93 MiB, increment: 1121.34 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 448.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.4 s, sys: 4.14 s, total: 56.6 s\n",
      "Wall time: 56.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_path = '/kaggle/input/vit/transformers/default/1/'\n",
    "hdf_test_path = '/kaggle/input/isic-2024-challenge/test-image.hdf5'\n",
    "hdf_train_path = '/kaggle/input/isic-2024-challenge/train-image.hdf5'\n",
    "\n",
    "if OWN_INSTANCE:\n",
    "    model_path = 'TobanDjan/vit'\n",
    "    hdf_test_path = 'data/test-image.hdf5'\n",
    "    hdf_train_path = 'data/train-image.hdf5'\n",
    "\n",
    "# Function to load images from encoded data\n",
    "def load_image_from_encoded_data(encoded_data):\n",
    "    image = Image.open(io.BytesIO(encoded_data))\n",
    "    return image.convert('RGB')\n",
    "\n",
    "# Define a custom Dataset for the HDF5 images\n",
    "class HDF5TestDataset(Dataset):\n",
    "    def __init__(self, image_data, ids, transform=None):\n",
    "        self.image_data = image_data\n",
    "        self.ids = ids\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_data = self.image_data[idx]\n",
    "        image = load_image_from_encoded_data(image_data)\n",
    "        #imshow(image)\n",
    "        #plt.show()\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # print(image.element_size() * image.nelement())\n",
    "        # 602112 B = 0.574 MB\n",
    "        return image, self.ids[idx]\n",
    "\n",
    "def get_dataset(hdf_file_path):\n",
    "    with h5py.File(hdf_file_path, 'r') as f:\n",
    "        image_data = [f[image_id][()] for image_id in tqdm(f.keys())]\n",
    "        ids = list(f.keys())\n",
    "        dataset = HDF5TestDataset(image_data=image_data, ids=ids, transform=val_transform)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "%memit train_dataset = get_dataset(hdf_train_path)\n",
    "test_dataset = get_dataset(hdf_test_path)\n",
    "\n",
    "vars_preserve.update({'train_dataset', 'test_dataset', 'model_path', 'load_image_from_encoded_data'})\n",
    "reset_globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2629.48 MiB, increment: 65.44 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1567/1567 [23:34<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 5640.83 MiB, increment: 3012.99 MiB\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vars_preserved' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest = combine_vit_features(test_dataloader, test)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     55\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain = combine_vit_features(train_dataloader, train)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m \u001b[43mvars_preserved\u001b[49m\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     58\u001b[0m vars_preserved\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     59\u001b[0m reset_globals()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vars_preserved' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the test dataset and dataloader\n",
    "batch_size = 2 ** 9\n",
    "\n",
    "if OWN_INSTANCE:\n",
    "    batch_size = 2 ** 8\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "model = None\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = AutoModelForImageClassification.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "\n",
    "def combine_vit_features(dataloader, metadata):\n",
    "    NUM_FEATURES = 768\n",
    "    feat_cols = list(range(NUM_FEATURES))\n",
    "\n",
    "    table = metadata.values\n",
    "    table = np.hstack((table, np.zeros((table.shape[0], NUM_FEATURES + 1))))\n",
    "    columns = metadata.columns.values.tolist()\n",
    "    columns.append('vit_target')\n",
    "    columns.extend(feat_cols)\n",
    "\n",
    "    row_offset = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, batch_ids in tqdm(dataloader, total = len(dataloader)):\n",
    "            inputs = inputs.to(device)\n",
    "            # print(inputs.element_size() * inputs.nelement())\n",
    "            outputs = model(inputs, output_hidden_states = True)\n",
    "\n",
    "            proba = outputs.logits.cpu()\n",
    "            proba = softmax(proba, axis=1)[:, 1]\n",
    "\n",
    "            last_hidden_states = outputs.hidden_states[-1].cpu()\n",
    "            cls_features = last_hidden_states[:, 0, :].squeeze().numpy().tolist()\n",
    "\n",
    "            for i, isic_id in enumerate(batch_ids):\n",
    "                row_idx = row_offset + i\n",
    "                table[row_idx, -NUM_FEATURES - 1] = proba[i]\n",
    "                table[row_idx, -NUM_FEATURES:] = cls_features[i]\n",
    "\n",
    "            row_offset += len(batch_ids)\n",
    "\n",
    "    # metadata.reset_index(inplace = True)\n",
    "    return pd.DataFrame(table, columns = columns)\n",
    "\n",
    "%memit test = combine_vit_features(test_dataloader, test)\n",
    "%memit train = combine_vit_features(train_dataloader, train)\n",
    "\n",
    "vars_preserve.remove('train_dataset')\n",
    "vars_preserve.remove('test_dataset')\n",
    "reset_globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition of a set of 2524572 objects. Total size = 5242670353 bytes.\n",
      " Index  Count   %     Size   % Cumulative  % Kind (class / dict of class)\n",
      "     0      2   0 2691928408  51 2691928408  51 pandas.core.frame.DataFrame\n",
      "     1 401062  16 2289308988  44 4981237396  95 numpy.bytes_\n",
      "     2 870868  34 94794775   2 5076032171  97 str\n",
      "     3 387627  15 29397744   1 5105429915  97 tuple\n",
      "     4 116619   5 20794036   0 5126223951  98 types.CodeType\n",
      "     5 179661   7 16425652   0 5142649603  98 bytes\n",
      "     6 102574   4 14770656   0 5157420259  98 function\n",
      "     7  49800   2 13060008   0 5170480267  99 dict (no owner)\n",
      "     8  11427   0 11104688   0 5181584955  99 type\n",
      "     9  29981   1  9994280   0 5191579235  99 list\n",
      "<3360 more rows. Type e.g. '_.more' to view.>\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "from guppy import hpy; h=hpy()\n",
    "print(h.heap())\n",
    "\n",
    "reset_globals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('target',axis=1)\n",
    "y = train['target']\n",
    "\n",
    "def pauc_above_tpr(solution, submission, min_tpr: float=0.80):\n",
    "    v_gt = abs(np.asarray(solution)-1)\n",
    "    v_pred = np.array([1.0 - x for x in submission])\n",
    "    max_fpr = abs(1-min_tpr)\n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    return partial_auc\n",
    "\n",
    "def Train_ML(model_factory, X, y, test_data):\n",
    "    # k-fold cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    test_predictions = [] \n",
    "    models = []\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(tqdm(skf.split(X, y), total=n_splits), 1):\n",
    "        # StratifiedKFold yields the indices from which we retrieve pandas metadata\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_val = y[train_index], y[test_index]\n",
    "        \n",
    "        model = model_factory()\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # record performance on all sets\n",
    "        y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "        train_pauc = pauc_above_tpr(y_train,y_train_pred_proba, min_tpr=0.8)\n",
    "        train_scores.append(train_pauc)\n",
    "\n",
    "        y_val_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "        val_pauc = pauc_above_tpr(y_val, y_val_pred_proba, min_tpr=0.8)\n",
    "        val_scores.append(val_pauc)\n",
    "        \n",
    "        # make predictions\n",
    "        y_test_pred_proba = model.predict_proba(test)[:, 1]\n",
    "        test_predictions.append(y_test_pred_proba)\n",
    "        \n",
    "        models.append(model)\n",
    "\n",
    "        print(f\"Fold {fold}: Train pAUC = {train_pauc:.4f}, Validation pAUC = {val_pauc:.4f}\")\n",
    "\n",
    "    # mean pauc on different folds' models\n",
    "    mean_train_pauc = np.mean(train_scores)\n",
    "    mean_val_pauc = np.mean(val_scores)\n",
    "\n",
    "    print(f\"\\nMean Train pAUC: {mean_train_pauc:.4f}\")\n",
    "    print(f\"Mean Validation pAUC: {mean_val_pauc:.4f}\")\n",
    "\n",
    "    # why would you want the \"model\"?\n",
    "    return model,test_predictions, models\n",
    "\n",
    "vars_preserve.update({'X', 'y', 'Train_ML', 'pauc_above_tpr'})\n",
    "reset_globals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████████████████▋                                                                                 | 1/3 [01:13<02:26, 73.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Train pAUC = 0.2000, Validation pAUC = 0.1769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|█████████████████████████████████████████████████████████████████████████████████▎                                        | 2/3 [02:27<01:13, 73.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: Train pAUC = 0.2000, Validation pAUC = 0.1724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [03:40<00:00, 73.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3: Train pAUC = 0.2000, Validation pAUC = 0.1729\n",
      "\n",
      "Mean Train pAUC: 0.2000\n",
      "Mean Validation pAUC: 0.1741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51min 58s, sys: 7.38 s, total: 52min 5s\n",
      "Wall time: 3min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def lgbm_factory():\n",
    "    params =  {\n",
    "            'objective': 'binary', 'colsample_bytree': 0.6852015051268027, 'max_depth': 4, \n",
    "            'learning_rate': 0.05714390301637632, 'n_estimators': 1010, 'subsample': 0.13326633837138008, \n",
    "            'lambda_l1': 1.4445754309498806e-08, 'lambda_l2': 0.11031259304642657, 'boosting_type': 'gbdt'\n",
    "                }\n",
    "    \n",
    "    Model = LGBMClassifier(**params,verbose=-1,random_state=SEED,\n",
    "                          extra_tree=True,max_bin=250,reg_alpha=0.1,reg_lambda=0.8\n",
    "                          )\n",
    "    return Model\n",
    "\n",
    "train_lgb, test_preds, all_models = Train_ML(lgbm_factory, X, y, test)\n",
    "\n",
    "vars_preserve.update({'train_lgb', 'test_preds', 'all_models'})\n",
    "reset_globals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████████████████▎                                                                                | 1/3 [01:58<03:57, 118.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Train pAUC = 0.1933, Validation pAUC = 0.1802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████████████████████████████████████████████████████████▋                                        | 2/3 [03:55<01:57, 117.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: Train pAUC = 0.1957, Validation pAUC = 0.1724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [05:51<00:00, 117.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3: Train pAUC = 0.1933, Validation pAUC = 0.1766\n",
      "\n",
      "Mean Train pAUC: 0.1941\n",
      "Mean Validation pAUC: 0.1764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 19512.98 MiB, increment: 5448.84 MiB\n",
      "CPU times: user 38min 57s, sys: 19min 2s, total: 58min\n",
      "Wall time: 5min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def cat_factory():\n",
    "    Cat_Model = CatBoostClassifier(verbose=0,random_state=SEED,\n",
    "                              iterations = 1000,\n",
    "                              learning_rate=0.01,\n",
    "                              objective = 'Logloss',\n",
    "                              boosting_type = 'Plain',\n",
    "                              bootstrap_type = 'Bernoulli',\n",
    "                              colsample_bylevel = 0.08656159895289164,\n",
    "                              subsample = 0.46623542352578917,\n",
    "                              depth=9,)\n",
    "    return Cat_Model\n",
    "\n",
    "%memit train_cat, cat_test_preds , Cat_all_models = Train_ML(cat_factory, X, y, test)\n",
    "\n",
    "vars_preserve.update({'train_cat', 'cat_test_preds', 'Cat_all_models'})\n",
    "reset_globals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████████████████▎                                                                                | 1/3 [01:43<03:27, 103.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Train pAUC = 0.1999, Validation pAUC = 0.1811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████████████████████████████████████████████████████████▋                                        | 2/3 [03:24<01:42, 102.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: Train pAUC = 0.1999, Validation pAUC = 0.1722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [05:06<00:00, 102.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3: Train pAUC = 0.1999, Validation pAUC = 0.1790\n",
      "\n",
      "Mean Train pAUC: 0.1999\n",
      "Mean Validation pAUC: 0.1774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 21456.12 MiB, increment: 6128.44 MiB\n",
      "CPU times: user 1h 14min 7s, sys: 15.2 s, total: 1h 14min 22s\n",
      "Wall time: 5min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def xgb_factory():\n",
    "    xgb_params2 = {\n",
    "        'objective': 'binary:logistic', 'colsample_bytree': 0.11756728710020253,'max_depth': 4, \n",
    "        'learning_rate': 0.009393224320850784,'n_estimators': 1227, 'subsample': 0.9589462514195692,\n",
    "        'lambda': 0.34216652262461505,'alpha': 1.150597512455824e-07\n",
    "                  }\n",
    "    \n",
    "    xgb_Model = XGBClassifier(**xgb_params2,random_state=SEED)\n",
    "    return xgb_Model\n",
    "\n",
    "%memit train_xgb, xgb_test_preds , xgb_all_models = Train_ML(xgb_factory, X, y, test)\n",
    "\n",
    "vars_preserve.update({'train_xgb', 'xgb_test_preds', 'xgb_all_models'})\n",
    "reset_globals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 676 ms, sys: 222 μs, total: 676 ms\n",
      "Wall time: 672 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sample_file = '/kaggle/input/isic-2024-challenge/sample_submission.csv'\n",
    "\n",
    "if OWN_INSTANCE:\n",
    "    sample_file = 'data/sample_submission.csv'\n",
    "    \n",
    "Sample = pd.read_csv(sample_file)\n",
    "\n",
    "lgb_test = np.mean(test_preds, axis=0)\n",
    "cat_test = np.mean(cat_test_preds, axis=0)\n",
    "xgb_test = np.mean(xgb_test_preds, axis=0)\n",
    "\n",
    "\n",
    "ensemble_preds = (lgb_test + cat_test + xgb_test) / 3\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    'isic_id': Sample['isic_id'],\n",
    "    'target': ensemble_preds\n",
    "})\n",
    "\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "sub.head()\n",
    "\n",
    "vars_preserve.update({'sub'})\n",
    "reset_globals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_models(framework, models):\n",
    "    for idx, model in enumerate(models):\n",
    "        joblib.dump(model, f'gradboost/{framework}_{idx}.joblib')\n",
    "\n",
    "dump_models(\"lgbm\", all_models)\n",
    "dump_models(\"catboost\", Cat_all_models)\n",
    "dump_models(\"xgb\", xgb_all_models)\n",
    "\n",
    "reset_globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AutoModelForImageClassification',\n",
      " 'BaseCrossValidator',\n",
      " 'BaseShuffleSplit',\n",
      " 'Binarizer',\n",
      " 'CatBoostClassifier',\n",
      " 'Cat_all_models',\n",
      " 'ConfusionMatrixDisplay',\n",
      " 'DataLoader',\n",
      " 'Dataset',\n",
      " 'DetCurveDisplay',\n",
      " 'DistanceMetric',\n",
      " 'FixedThresholdClassifier',\n",
      " 'FunctionTransformer',\n",
      " 'GridSearchCV',\n",
      " 'GroupKFold',\n",
      " 'GroupShuffleSplit',\n",
      " 'Image',\n",
      " 'In',\n",
      " 'KBinsDiscretizer',\n",
      " 'KFold',\n",
      " 'KernelCenterer',\n",
      " 'LGBMClassifier',\n",
      " 'LabelBinarizer',\n",
      " 'LabelEncoder',\n",
      " 'LearningCurveDisplay',\n",
      " 'LeaveOneGroupOut',\n",
      " 'LeaveOneOut',\n",
      " 'LeavePGroupsOut',\n",
      " 'LeavePOut',\n",
      " 'MaxAbsScaler',\n",
      " 'MinMaxScaler',\n",
      " 'MultiLabelBinarizer',\n",
      " 'Normalizer',\n",
      " 'OWN_INSTANCE',\n",
      " 'OneHotEncoder',\n",
      " 'OrdinalEncoder',\n",
      " 'Out',\n",
      " 'ParameterGrid',\n",
      " 'ParameterSampler',\n",
      " 'PolynomialFeatures',\n",
      " 'PowerTransformer',\n",
      " 'PrecisionRecallDisplay',\n",
      " 'PredefinedSplit',\n",
      " 'PredictionErrorDisplay',\n",
      " 'QuantileTransformer',\n",
      " 'RandomizedSearchCV',\n",
      " 'RepeatedKFold',\n",
      " 'RepeatedStratifiedKFold',\n",
      " 'RobustScaler',\n",
      " 'RocCurveDisplay',\n",
      " 'SEED',\n",
      " 'ShuffleSplit',\n",
      " 'SplineTransformer',\n",
      " 'StandardScaler',\n",
      " 'StratifiedGroupKFold',\n",
      " 'StratifiedKFold',\n",
      " 'StratifiedShuffleSplit',\n",
      " 'TargetEncoder',\n",
      " 'TimeSeriesSplit',\n",
      " 'Train_ML',\n",
      " 'TunedThresholdClassifierCV',\n",
      " 'ValidationCurveDisplay',\n",
      " 'X',\n",
      " 'XGBClassifier',\n",
      " '_',\n",
      " '__',\n",
      " '___',\n",
      " '__builtin__',\n",
      " '__builtins__',\n",
      " '__doc__',\n",
      " '__loader__',\n",
      " '__name__',\n",
      " '__package__',\n",
      " '__session__',\n",
      " '__spec__',\n",
      " '_dh',\n",
      " '_i',\n",
      " '_i1',\n",
      " '_i23',\n",
      " '_ih',\n",
      " '_ii',\n",
      " '_iii',\n",
      " '_oh',\n",
      " 'accuracy_score',\n",
      " 'add_dummy_feature',\n",
      " 'adjusted_mutual_info_score',\n",
      " 'adjusted_rand_score',\n",
      " 'all_models',\n",
      " 'auc',\n",
      " 'average_precision_score',\n",
      " 'balanced_accuracy_score',\n",
      " 'binarize',\n",
      " 'brier_score_loss',\n",
      " 'calinski_harabasz_score',\n",
      " 'cat_test_preds',\n",
      " 'ce',\n",
      " 'check_cv',\n",
      " 'check_scoring',\n",
      " 'class_likelihood_ratios',\n",
      " 'classification_report',\n",
      " 'cluster',\n",
      " 'cohen_kappa_score',\n",
      " 'completeness_score',\n",
      " 'confusion_matrix',\n",
      " 'consensus_score',\n",
      " 'coverage_error',\n",
      " 'cross_val_predict',\n",
      " 'cross_val_score',\n",
      " 'cross_validate',\n",
      " 'd2_absolute_error_score',\n",
      " 'd2_log_loss_score',\n",
      " 'd2_pinball_score',\n",
      " 'd2_tweedie_score',\n",
      " 'davies_bouldin_score',\n",
      " 'dcg_score',\n",
      " 'det_curve',\n",
      " 'env_globals',\n",
      " 'euclidean_distances',\n",
      " 'exit',\n",
      " 'explained_variance_score',\n",
      " 'f1_score',\n",
      " 'fbeta_score',\n",
      " 'fowlkes_mallows_score',\n",
      " 'gc',\n",
      " 'get_ipython',\n",
      " 'get_scorer',\n",
      " 'get_scorer_names',\n",
      " 'h5py',\n",
      " 'hamming_loss',\n",
      " 'hinge_loss',\n",
      " 'homogeneity_completeness_v_measure',\n",
      " 'homogeneity_score',\n",
      " 'imshow',\n",
      " 'io',\n",
      " 'jaccard_score',\n",
      " 'joblib',\n",
      " 'label_binarize',\n",
      " 'label_ranking_average_precision_score',\n",
      " 'label_ranking_loss',\n",
      " 'learning_curve',\n",
      " 'load_image_from_encoded_data',\n",
      " 'log_loss',\n",
      " 'make_scorer',\n",
      " 'matthews_corrcoef',\n",
      " 'max_error',\n",
      " 'maxabs_scale',\n",
      " 'mean_absolute_error',\n",
      " 'mean_absolute_percentage_error',\n",
      " 'mean_gamma_deviance',\n",
      " 'mean_pinball_loss',\n",
      " 'mean_poisson_deviance',\n",
      " 'mean_squared_error',\n",
      " 'mean_squared_log_error',\n",
      " 'mean_tweedie_deviance',\n",
      " 'median_absolute_error',\n",
      " 'minmax_scale',\n",
      " 'model_path',\n",
      " 'multilabel_confusion_matrix',\n",
      " 'mutual_info_score',\n",
      " 'n_splits',\n",
      " 'nan_euclidean_distances',\n",
      " 'ndcg_score',\n",
      " 'normalize',\n",
      " 'normalized_mutual_info_score',\n",
      " 'np',\n",
      " 'open',\n",
      " 'os',\n",
      " 'pair_confusion_matrix',\n",
      " 'pairwise_distances',\n",
      " 'pairwise_distances_argmin',\n",
      " 'pairwise_distances_argmin_min',\n",
      " 'pairwise_distances_chunked',\n",
      " 'pairwise_kernels',\n",
      " 'pauc_above_tpr',\n",
      " 'pd',\n",
      " 'permutation_test_score',\n",
      " 'plt',\n",
      " 'power_transform',\n",
      " 'pprint',\n",
      " 'precision_recall_curve',\n",
      " 'precision_recall_fscore_support',\n",
      " 'precision_score',\n",
      " 'quantile_transform',\n",
      " 'quit',\n",
      " 'r2_score',\n",
      " 'rand_score',\n",
      " 'recall_score',\n",
      " 'reset_globals',\n",
      " 'robust_scale',\n",
      " 'roc_auc_score',\n",
      " 'roc_curve',\n",
      " 'root_mean_squared_error',\n",
      " 'root_mean_squared_log_error',\n",
      " 'scale',\n",
      " 'silhouette_samples',\n",
      " 'silhouette_score',\n",
      " 'softmax',\n",
      " 'sub',\n",
      " 'sys',\n",
      " 'test',\n",
      " 'test_dataset',\n",
      " 'test_preds',\n",
      " 'top_k_accuracy_score',\n",
      " 'torch',\n",
      " 'tqdm',\n",
      " 'train',\n",
      " 'train_cat',\n",
      " 'train_dataset',\n",
      " 'train_lgb',\n",
      " 'train_test_split',\n",
      " 'train_xgb',\n",
      " 'transforms',\n",
      " 'v_measure_score',\n",
      " 'validation_curve',\n",
      " 'vars_preserve',\n",
      " 'warnings',\n",
      " 'xgb_all_models',\n",
      " 'xgb_test_preds',\n",
      " 'y',\n",
      " 'zero_one_loss']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(sorted(list(globals().keys())))\n",
    "\n",
    "reset_globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9094797,
     "sourceId": 63056,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 112702,
     "modelInstanceId": 88477,
     "sourceId": 105577,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
